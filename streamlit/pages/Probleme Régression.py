import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from plotly.subplots import make_subplots
import plotly.express as px
import plotly.graph_objects as go
from scipy import stats
import nbformat
from scipy.stats import pearsonr, f_oneway, kruskal
from sklearn.model_selection import train_test_split, GridSearchCV
import collections
import tabulate
import os
from scipy.stats import chi2_contingency
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import sklearn.datasets
from sklearn.datasets import load_diabetes
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
import xgboost as xgb
from sklearn.linear_model import LinearRegression, Ridge, Lasso
import plotly.figure_factory as ff
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import shap
from sklearn.model_selection import cross_val_score, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
import io
import pickle
import joblib
import time
from concurrent.futures import ThreadPoolExecutor
import threading

# Supprimer les avertissements
warnings.filterwarnings('ignore', category=UserWarning, module='seaborn')
warnings.filterwarnings('ignore', category=FutureWarning, module='seaborn')
warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=FutureWarning, module='pandas')

# Fonction pour afficher les DataFrames de mani√®re s√ªre
def safe_dataframe_display(df, title="DataFrame", max_rows=1000):
    """Affiche un DataFrame de mani√®re s√ªre en g√©rant les probl√®mes de s√©rialisation Arrow"""
    try:
        if len(df) > max_rows:
            st.warning(f"DataFrame trop volumineux ({len(df)} lignes). Affichage des {max_rows} premi√®res lignes.")
            df_display = df.head(max_rows).copy()
        else:
            df_display = df.copy()
        
        # Convertir les colonnes probl√©matiques
        for col in df_display.columns:
            if df_display[col].dtype == 'object':
                try:
                    df_display[col] = df_display[col].astype(str)
                except:
                    pass
        
        st.dataframe(df_display)
    except Exception as e:
        st.error(f"Erreur lors de l'affichage du {title}: {e}")
        st.write(f"**{title} - Informations de base :**")
        st.write(f"Forme : {df.shape}")
        st.write(f"Colonnes : {list(df.columns)}")
        st.write("√âchantillon des donn√©es :")
        st.text(str(df.head()))

# Fonction pour calculer le Cramer's V
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1)) / (n-1))
    rcorr = r - ((r-1)**2) / (n-1)
    kcorr = k - ((k-1)**2) / (n-1)
    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))

# Fonction optimis√©e pour charger les mod√®les avec cache
@st.cache_data(ttl=3600)  # Cache pendant 1 heure
def load_models_and_scalers_cached(models_dir="saved_models_regression"):
    """Version mise en cache de la fonction de chargement des mod√®les"""
    return load_models_and_scalers(models_dir)

# Fonction pour charger les mod√®les et scalers
def load_models_and_scalers(models_dir="saved_models_regression"):
    """Charge uniquement les mod√®les de ML depuis le dossier sp√©cifi√©"""
    loaded_models = {}
    loaded_scalers = {}
    
    if not os.path.exists(models_dir):
        st.error(f"Le dossier '{models_dir}' n'existe pas.")
        return {}, {}
    
    try:
        files = os.listdir(models_dir)
        pkl_files = [f for f in files if f.endswith('.pkl')]
        
        # Types d'objets qui sont des mod√®les de ML
        ml_model_types = [
            'LinearRegression', 'Ridge', 'Lasso', 'ElasticNet',
            'RandomForestRegressor', 'GradientBoostingRegressor', 
            'XGBRegressor', 'LGBMRegressor', 'CatBoostRegressor',
            'SVR', 'DecisionTreeRegressor', 'ExtraTreesRegressor',
            'AdaBoostRegressor', 'BaggingRegressor', 'VotingRegressor',
            'StackingRegressor', 'MLPRegressor', 'KNeighborsRegressor'
        ]
        
        def load_single_model(pkl_file):
            file_path = os.path.join(models_dir, pkl_file)
            try:
                loaded_object = joblib.load(file_path)
                object_type = type(loaded_object).__name__
                
                if object_type in ml_model_types:
                    model_name = pkl_file.replace('.pkl', '')
                    if 'conventionnel' in model_name:
                        display_name = model_name.replace('reg_conventionnel_', '').replace('_', ' ').title()
                    elif 'gridsearch' in model_name:
                        display_name = model_name.replace('reg_gridsearch_', '').replace('_', ' ').title() + ' (GridSearch)'
                    else:
                        display_name = model_name.replace('reg_', '').replace('_', ' ').title()
                    
                    return ('model', display_name, loaded_object)
                
                elif 'scaler' in pkl_file.lower():
                    scaler_name = pkl_file.replace('.pkl', '').replace('_scaler', '').replace('scaler_', '')
                    return ('scaler', scaler_name, loaded_object)
                
            except Exception as e:
                return ('error', pkl_file, str(e))
            
            return None
        
        # Chargement parall√®le des mod√®les
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(load_single_model, pkl_files))
        
        for result in results:
            if result is None:
                continue
            
            result_type, name, obj = result
            
            if result_type == 'model':
                loaded_models[name] = obj
            elif result_type == 'scaler':
                loaded_scalers[name] = obj
            elif result_type == 'error':
                st.warning(f"‚ö†Ô∏è Impossible de charger {name}: {obj}")
        
        return loaded_models, loaded_scalers
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement : {str(e)}")
        return {}, {}

# Fonction optimis√©e pour √©valuer les mod√®les
def evaluate_models_parallel(models_dict, X_test, y_test):
    """√âvalue les mod√®les en parall√®le pour am√©liorer les performances"""
    
    def evaluate_single_model(model_item):
        name, model = model_item
        try:
            y_pred = model.predict(X_test)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            return name, {'MAE': mae, 'R2': r2}
        except Exception as e:
            return name, {'MAE': np.nan, 'R2': np.nan, 'error': str(e)}
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(evaluate_single_model, models_dict.items()))
    
    return dict(results)

# Fonction pour cr√©er le tableau Plotly am√©lior√©
def create_enhanced_plotly_table(df_results, title, best_model_name=None):
    """Cr√©e un tableau Plotly avec mise en surbrillance et code couleur"""
    
    # Pr√©parer les donn√©es
    df_display = df_results.copy()
    
    # Cr√©er le tableau de base
    fig = go.Figure(data=[go.Table(
        header=dict(
            values=['<b>Mod√®le</b>', '<b>MAE</b>', '<b>R¬≤</b>'],
            fill_color='darkslategray',
            font=dict(color='white', size=14),
            align='left'
        ),
        cells=dict(
            values=[
                df_display['Mod√®le'],
                df_display['MAE'],
                df_display['R¬≤']
            ],
            fill_color=[
                ['white'] * len(df_display),  # Colonne Mod√®le
                ['white'] * len(df_display),  # Colonne MAE
                ['white'] * len(df_display)   # Colonne R¬≤
            ],
            align='left',
            font=dict(size=12)
        )
    )])
    
    # Appliquer les couleurs
    for i, row in df_display.iterrows():
        # Mise en surbrillance du meilleur mod√®le
        if best_model_name and row['Mod√®le'] == best_model_name:
            for col in range(3):
                fig.data[0].cells.fill.color[col][i] = 'gold'
        
        # Code couleur pour R¬≤
        r2_val = row['R¬≤']
        if r2_val != 'Erreur':
            try:
                r2_float = float(r2_val)
                if r2_float > 0.95:
                    color = 'lightgreen'
                elif r2_float < 0.7:
                    color = 'lightcoral'
                elif r2_float < 0.9:
                    color = 'lightyellow'
                else:
                    color = 'lightblue'
                
                # Appliquer la couleur √† la colonne R¬≤ (index 2)
                if best_model_name and row['Mod√®le'] != best_model_name:
                    fig.data[0].cells.fill.color[2][i] = color
            except:
                pass
    
    # Mise en forme du tableau
    fig.update_layout(
        title=f"<b>{title}</b>",
        title_x=0.5,
        margin=dict(l=20, r=20, t=60, b=20),
        height=300
    )
    
    return fig

# Page de r√©gression lin√©aire avec onglets
def show_linear_regression():
    st.markdown('<div id="top"></div>', unsafe_allow_html=True)
    
    st.title("Identification des facteurs favorisant les √©missions de CO2")
    st.markdown("Exploration et mod√©lisation de la relation entre les caract√©ristiques et les √©missions de CO2.")
    st.markdown("---")

    # Initialiser les variables
    dataframes = None
    df = None
    df_final = None
    df_final_ml = None
    numerical_cols = None
    X_train, X_test, y_train, y_test = None, None, None, None
    feature_columns = None
    lr_model = None

    tabs = st.tabs([
        "Chargement des Donn√©es",
        "Preprocessing", 
        "Visualisation",
        "Feature Engineering",
        "Mod√©lisation ML",
        "Feature Importance",
        "Conclusions / Recommandations"
    ])

    # ----------------------------------------------------------------
    # Onglet Chargement des Donn√©es (index 0)
    # ----------------------------------------------------------------
    with tabs[0]:
        st.markdown("Chargement des fichiers sources et analyse exploratoire")
        
        dataframes = {}
        try:
            if not os.path.exists("raw_data"):
                st.error("Le dossier 'raw_data' n'a pas √©t√© trouv√©. Veuillez cr√©er un dossier nomm√© 'raw_data' et y placer vos fichiers CSV.")
                st.stop()

            csv_files = [f for f in os.listdir("raw_data") if f.endswith(".csv")]
            if not csv_files:
                st.error("Aucun fichier CSV trouv√© dans le dossier 'raw_data'.")
                st.stop()

            # √âTAPE 1: Chargement des fichiers
            st.subheader(" √âtape 1 : Chargement des Fichiers CSV")
            
            loading_results = []
            
            for filename in csv_files:
                year = filename.split(".")[0]
                filepath = os.path.join("raw_data", filename)
                
                try:
                    df_temp = pd.read_csv(filepath, sep=";", encoding="ISO-8859-1")
                    dataframes[year] = df_temp
                    
                    memory_usage = df_temp.memory_usage(deep=True).sum() / 1024**2
                    
                    loading_results.append({
                        'Fichier': filename,
                        'Ann√©e': year,
                        'Lignes': f"{len(df_temp):,}",
                        'Colonnes': len(df_temp.columns),
                        'Taille (MB)': f"{memory_usage:.2f}",
                        'Statut': '‚úÖ Succ√®s'
                    })
                    
                except Exception as e:
                    loading_results.append({
                        'Fichier': filename,
                        'Ann√©e': year,
                        'Lignes': 'N/A',
                        'Colonnes': 'N/A',
                        'Taille (MB)': 'N/A',
                        'Statut': f'‚ùå Erreur: {str(e)[:50]}...'
                    })
            
            # Afficher le tableau r√©capitulatif
            df_loading_summary = pd.DataFrame(loading_results)
            safe_dataframe_display(df_loading_summary, "R√©sum√© du chargement")
            
            # M√©triques globales
            successful_loads = len([r for r in loading_results if '‚úÖ' in r['Statut']])
            total_rows = sum(len(df) for df in dataframes.values())
            total_memory = sum(df.memory_usage(deep=True).sum() for df in dataframes.values()) / 1024**2
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìÅ Fichiers charg√©s", f"{successful_loads}/{len(csv_files)}")
            with col2:
                st.metric("üìä Total lignes", f"{total_rows:,}")
            with col3:
                st.metric("üìÖ Ann√©es couvertes", len(dataframes))
            with col4:
                st.metric("üíæ M√©moire totale", f"{total_memory:.1f} MB")

            st.success(f"‚úÖ Chargement r√©ussi de {len(dataframes)} DataFrame(s) depuis le dossier 'raw_data' !")

            # √âTAPE 2: Exploration par DataFrame
            st.markdown("---")
            st.subheader(" √âtape 2 : Exploration par Ann√©e")
            
            if dataframes:
                selected_year = st.selectbox(
                    "üéØ S√©lectionnez une ann√©e pour l'exploration d√©taill√©e :",
                    options=sorted(dataframes.keys()),
                    key="year_selector"
                )
                
                if selected_year:
                    df_selected = dataframes[selected_year]
                    
                    # Onglets pour diff√©rents types d'exploration
                    explore_tabs = st.tabs([
                        "üìä Vue d'ensemble", 
                        "üìà Statistiques", 
                        "üîç Colonnes", 
                        "üìã √âchantillon"
                    ])
                    
                    # Sous-onglet: Vue d'ensemble
                    with explore_tabs[0]:
                        st.markdown(f"### üìä Vue d'ensemble - Ann√©e {selected_year}")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric("üìè Dimensions", f"{df_selected.shape[0]} √ó {df_selected.shape[1]}")
                        
                        with col2:
                            memory_mb = df_selected.memory_usage(deep=True).sum() / 1024**2
                            st.metric("üíæ M√©moire", f"{memory_mb:.2f} MB")
                        
                        with col3:
                            null_count = df_selected.isnull().sum().sum()
                            st.metric("‚ùì Valeurs manquantes", f"{null_count:,}")
                        
                        with col4:
                            duplicates = df_selected.duplicated().sum()
                            st.metric("üîÑ Doublons", f"{duplicates:,}")
                        
                        # Informations sur les types de donn√©es - Tableau seulement
                        st.markdown("#### üìã R√©partition des Types de Donn√©es")
                        
                        type_counts = df_selected.dtypes.value_counts()
                        
                        # Tableau √©tir√© sur toute la largeur
                        type_details = pd.DataFrame({
                            'Type': type_counts.index.astype(str),
                            'Nombre de colonnes': type_counts.values,
                            'Pourcentage': (type_counts.values / len(df_selected.columns) * 100).round(1)
                        })
                        safe_dataframe_display(type_details, "D√©tail des types")

                    # Sous-onglet: Statistiques
                    with explore_tabs[1]:
                        st.markdown(f"### üìà Statistiques Descriptives - Ann√©e {selected_year}")
                        
                        numeric_cols = df_selected.select_dtypes(include=[np.number]).columns
                        categorical_cols = df_selected.select_dtypes(include=['object']).columns
                        
                        if len(numeric_cols) > 0:
                            st.markdown("#### üî¢ Variables Num√©riques")
                            
                            numeric_stats = df_selected[numeric_cols].describe()
                            safe_dataframe_display(numeric_stats, "Statistiques num√©riques")
                            
                            if len(numeric_cols) > 1:
                                missing_numeric = df_selected[numeric_cols].isnull().sum()
                                missing_numeric = missing_numeric[missing_numeric > 0]
                                
                                if len(missing_numeric) > 0:
                                    fig_missing_num = px.bar(
                                        x=missing_numeric.index,
                                        y=missing_numeric.values,
                                        title="Valeurs manquantes - Variables num√©riques",
                                        labels={'x': 'Variables', 'y': 'Nombre de valeurs manquantes'},
                                        color=missing_numeric.values,
                                        color_continuous_scale='Reds'
                                    )
                                    fig_missing_num.update_layout(height=400)
                                    st.plotly_chart(fig_missing_num, use_container_width=True)
                        
                        if len(categorical_cols) > 0:
                            st.markdown("#### üè∑Ô∏è Variables Cat√©gorielles")
                            
                            cat_summary = []
                            for col in categorical_cols:
                                cat_summary.append({
                                    'Variable': col,
                                    'Valeurs uniques': df_selected[col].nunique(),
                                    'Valeurs manquantes': df_selected[col].isnull().sum(),
                                    'Mode': df_selected[col].mode().iloc[0] if not df_selected[col].mode().empty else 'N/A',
                                    'Fr√©quence du mode': df_selected[col].value_counts().iloc[0] if not df_selected[col].empty else 0
                                })
                            
                            df_cat_summary = pd.DataFrame(cat_summary)
                            safe_dataframe_display(df_cat_summary, "R√©sum√© variables cat√©gorielles")

                    # Sous-onglet: Colonnes
                    with explore_tabs[2]:
                        st.markdown(f"### üîç Analyse des Colonnes - Ann√©e {selected_year}")
                        
                        col_analysis = []
                        for col in df_selected.columns:
                            unique_count = df_selected[col].nunique()
                            null_count = df_selected[col].isnull().sum()
                            null_pct = (null_count / len(df_selected)) * 100
                            
                            if df_selected[col].dtype in ['int64', 'float64']:
                                var_type = "üî¢ Num√©rique"
                            else:
                                var_type = "üè∑Ô∏è Cat√©gorielle"
                            
                            if unique_count == 1:
                                cardinality = "üîí Constante"
                            elif unique_count == len(df_selected):
                                cardinality = "üîë Identifiant"
                            elif unique_count / len(df_selected) > 0.95:
                                cardinality = "üìà Tr√®s haute"
                            elif unique_count > 50:
                                cardinality = "üìä Haute"
                            elif unique_count > 10:
                                cardinality = "üìã Moyenne"
                            else:
                                cardinality = "üìù Faible"
                            
                            col_analysis.append({
                                'Colonne': col,
                                'Type': var_type,
                                'Cardinalit√©': cardinality,
                                'Valeurs uniques': unique_count,
                                'Valeurs manquantes': null_count,
                                '% manquantes': round(null_pct, 2),
                                'Ratio unicit√©': round(unique_count / len(df_selected), 4)
                            })
                        
                        df_col_analysis = pd.DataFrame(col_analysis)
                        safe_dataframe_display(df_col_analysis, "Analyse d√©taill√©e des colonnes")
                        
                        st.markdown("#### üéØ Analyse Cibl√©e")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            high_missing = df_col_analysis[df_col_analysis['% manquantes'] > 10]
                            if not high_missing.empty:
                                st.warning("‚ö†Ô∏è **Colonnes avec > 10% de valeurs manquantes:**")
                                safe_dataframe_display(
                                    high_missing[['Colonne', '% manquantes']].sort_values('% manquantes', ascending=False),
                                    "Colonnes probl√©matiques"
                                )
                            else:
                                st.success("‚úÖ Aucune colonne avec > 10% de valeurs manquantes")
                        
                        with col2:
                            low_variance = df_col_analysis[df_col_analysis['Valeurs uniques'] <= 2]
                            if not low_variance.empty:
                                st.info("‚ÑπÔ∏è **Colonnes avec peu de variance:**")
                                safe_dataframe_display(
                                    low_variance[['Colonne', 'Valeurs uniques', 'Cardinalit√©']],
                                    "Colonnes faible variance"
                                )
                            else:
                                st.success("‚úÖ Toutes les colonnes ont une variance suffisante")

                    # Sous-onglet: √âchantillon
                    with explore_tabs[3]:
                        st.markdown(f"### üìã √âchantillon des Donn√©es - Ann√©e {selected_year}")
                        
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            sample_size = st.selectbox(
                                "üìè Nombre de lignes √† afficher:",
                                [5, 10, 20, 50, 100],
                                index=1,
                                key=f"sample_size_{selected_year}"
                            )
                        
                        with col2:
                            sample_type = st.selectbox(
                                "üéØ Type d'√©chantillon:",
                                ["Premi√®res lignes", "Derni√®res lignes", "√âchantillon al√©atoire"],
                                key=f"sample_type_{selected_year}"
                            )
                        
                        with col3:
                            show_info = st.checkbox(
                                "üìä Afficher info() du DataFrame",
                                value=False,
                                key=f"show_info_{selected_year}"
                            )
                        
                        if sample_type == "Premi√®res lignes":
                            sample_df = df_selected.head(sample_size)
                            st.markdown(f"**üîù {sample_size} premi√®res lignes:**")
                        elif sample_type == "Derni√®res lignes":
                            sample_df = df_selected.tail(sample_size)
                            st.markdown(f"**üîö {sample_size} derni√®res lignes:**")
                        else:
                            sample_df = df_selected.sample(n=min(sample_size, len(df_selected)), random_state=42)
                            st.markdown(f"**üé≤ {sample_size} lignes al√©atoires:**")
                        
                        safe_dataframe_display(sample_df, f"√âchantillon {selected_year}")
                        
                        if show_info:
                            st.markdown("#### üìä Informations D√©taill√©es du DataFrame")
                            
                            buffer = io.StringIO()
                            df_selected.info(buf=buffer)
                            info_str = buffer.getvalue()
                            
                            st.text(info_str)

            # √âTAPE 3: Comparaison entre ann√©es
            st.markdown("---")
            st.subheader(" √âtape 3 : Comparaison entre Ann√©es")
            
            if len(dataframes) > 1:
                st.markdown("#### üìä Analyse comparative")
                
                comparison_data = []
                for year, df_year in sorted(dataframes.items()):
                    comparison_data.append({
                        'Ann√©e': year,
                        'Lignes': f"{len(df_year):,}",
                        'Colonnes': len(df_year.columns),
                        'M√©moire (MB)': f"{df_year.memory_usage(deep=True).sum() / 1024**2:.2f}",
                        'Valeurs manquantes': f"{df_year.isnull().sum().sum():,}",
                        'Doublons': f"{df_year.duplicated().sum():,}"
                    })
                
                df_comparison = pd.DataFrame(comparison_data)
                safe_dataframe_display(df_comparison, "Comparaison par ann√©e")
                
                # Analyse des colonnes communes et diff√©rentes
                all_columns = set()
                for df_year in dataframes.values():
                    all_columns.update(df_year.columns)
                
                common_columns = set(dataframes[list(dataframes.keys())[0]].columns)
                for df_year in list(dataframes.values())[1:]:
                    common_columns = common_columns.intersection(set(df_year.columns))
                
                if len(common_columns) < len(all_columns):
                    st.markdown("#### üîç Analyse des Diff√©rences de Colonnes")
                    
                    diff_analysis = []
                    for year, df_year in dataframes.items():
                        missing_cols = all_columns - set(df_year.columns)
                        extra_cols = set(df_year.columns) - common_columns
                        
                        diff_analysis.append({
                            'Ann√©e': year,
                            'Colonnes manquantes': len(missing_cols),
                            'Colonnes sp√©cifiques': len(extra_cols),
                            'D√©tail manquantes': ', '.join(list(missing_cols)[:3]) + ('...' if len(missing_cols) > 3 else ''),
                            'D√©tail sp√©cifiques': ', '.join(list(extra_cols)[:3]) + ('...' if len(extra_cols) > 3 else '')
                        })
                    
                    df_diff_analysis = pd.DataFrame(diff_analysis)
                    safe_dataframe_display(df_diff_analysis, "Diff√©rences de colonnes")
            
            else:
                st.info("‚ÑπÔ∏è Une seule ann√©e charg√©e. Comparaison non disponible.")

        except FileNotFoundError:
            st.error("Le dossier 'raw_data' n'a pas √©t√© trouv√©. Veuillez cr√©er un dossier nomm√© 'raw_data' et y placer vos fichiers CSV.")
        except Exception as e:
            st.error(f"Une erreur s'est produite lors du chargement des donn√©es : {e}")

        # Informations sur les prochaines √©tapes
        st.markdown("---")
        st.info("""
        **üöÄ Prochaines √©tapes :**
        
        Naviguez vers l'onglet **Preprocessing** pour :
        -  Nettoyer et standardiser les donn√©es
        -  Harmoniser les colonnes entre les ann√©es
        -  G√©rer les valeurs manquantes
        -  Pr√©parer les donn√©es pour l'analyse
        -  Cr√©er le DataFrame final unifi√©
        """)

    # ----------------------------------------------------------------
    # Onglet preprocessing (index 1)
    # ----------------------------------------------------------------
    with tabs[1]:
        st.markdown("Traitement des donn√©es et cr√©ation de la 1√®re phase du dataframe cible")
       
        if dataframes is None or not dataframes:
            st.info("‚ö†Ô∏è Veuillez d'abord charger les donn√©es dans l'onglet 'Chargement des Donn√©es'.")
        else:
            processed_dataframes = {year: df_year.copy() for year, df_year in dataframes.items()}

            # √âTAPE 1: Nettoyage initial des doublons
            st.subheader(" √âtape 1 : Suppression des Doublons")
            
            st.markdown("#### üîç D√©tails de la suppression des doublons")
            
            # Cr√©er un tableau r√©capitulatif
            duplicates_data = []
            total_duplicates_removed = 0
            
            for year, df_cleaning in sorted(processed_dataframes.items()):
                longueur_initiale = len(df_cleaning)
                nb_lignes_dupliquees = df_cleaning.duplicated().sum()
                
                df_cleaning.drop_duplicates(inplace=True)
                longueur_finale = len(df_cleaning)
                removed = longueur_initiale - longueur_finale
                total_duplicates_removed += removed
                
                df_cleaning.reset_index(drop=True, inplace=True)
                processed_dataframes[year] = df_cleaning
                
                duplicates_data.append({
                    'Ann√©e': year,
                    'Lignes initiales': f"{longueur_initiale:,}",
                    'Doublons d√©tect√©s': nb_lignes_dupliquees,
                    'Lignes finales': f"{longueur_finale:,}",
                    'Lignes supprim√©es': removed,
                    'Statut': '‚úÖ Nettoy√©' if removed > 0 else '‚ÑπÔ∏è Aucun doublon'
                })
            
            # Afficher le tableau
            df_duplicates_summary = pd.DataFrame(duplicates_data)
            safe_dataframe_display(df_duplicates_summary, "R√©sum√© suppression des doublons")
            
            # R√©sum√© global
            st.markdown("#### üìä R√©sum√© Global")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("üóëÔ∏è Total doublons supprim√©s", total_duplicates_removed)
            with col2:
                total_rows = sum(len(df) for df in processed_dataframes.values())
                st.metric("üìà Total lignes conserv√©es", f"{total_rows:,}")

            st.markdown("---")

            # √âTAPE 2: Ajout des colonnes manquantes
            st.subheader(" √âtape 2 : Harmonisation des Colonnes")
            
            st.markdown("#### ‚ûï Ajout des colonnes manquantes")
            if '2015' in processed_dataframes:
                cols_added = []
                
                if "Carrosserie" not in processed_dataframes['2015'].columns:
                    processed_dataframes['2015']["Carrosserie"] = np.nan
                    cols_added.append("Carrosserie")
                
                if "gamme" not in processed_dataframes['2015'].columns:
                    processed_dataframes['2015']["gamme"] = np.nan
                    cols_added.append("gamme")
                
                if cols_added:
                    st.success(f"‚úÖ Colonnes ajout√©es √† 2015 : {', '.join(cols_added)}")
                    for col in cols_added:
                        st.info(f"üìã '{col}' initialis√©e avec des valeurs NaN dans 2015")
                else:
                    st.info("‚ÑπÔ∏è Toutes les colonnes requises existent d√©j√† dans 2015")
            else:
                st.warning("‚ö†Ô∏è DataFrame 2015 non trouv√©")

            st.markdown("---")

            # Ajout de la colonne 'ann√©e' avec message apr√®s le tableau
            st.markdown("#### üìÖ Ajout de la colonne 'ann√©e'")
            for year, df_year in processed_dataframes.items():
                df_year["ann√©e"] = int(year)
            
            year_counts = {}
            for year, df_year in processed_dataframes.items():
                year_counts[year] = len(df_year)
            
            df_year_summary = pd.DataFrame(list(year_counts.items()), 
                                         columns=['Ann√©e', 'Nombre de lignes'])
            safe_dataframe_display(df_year_summary, "R√©partition par ann√©e")
            
            # Message apr√®s le tableau
            st.success("‚úÖ Colonne 'ann√©e' ajout√©e √† tous les DataFrames")

            st.markdown("---")

            # √âTAPE 3: Standardisation des noms de colonnes
            st.subheader(" √âtape 3 : Mapping et uniformisation")
            
            column_mapping = {
                "Bo√Æte de vitesse": "typ_boite_nb_rapp",
                "Carburant": "typ_crb",
                "Carrosserie": "Carrosserie",
                "Champ V9": "champ_v9",
                "CNIT" : "cnit",
                "CO type I (g/km)": "co_typ_1",
                "co_typ_1": "co_typ_1",
                "CO2 (g/km)": "co2",
                "co2": "co2",
                "co2_mixte": "co2",
                "cod_cbr": "typ_crb",
                "conso_urb_93": "conso_urb",
                "Consommation extra-urbaine (l/100km)": "conso_exurb",
                "Consommation mixte (l/100km)": "conso_mixte",
                "Consommation urbaine (l/100km)": "conso_urb",
                "Date de mise √† jour": "date_maj",
                "D√©signation commerciale": "dscom",
                "dscom": "dscom",
                "energ": "typ_crb",
                "HC (g/km)": "hc",
                "HC+NOX (g/km)": "hcnox",
                "hibride": "hybride",
                "Hybride" : "hybride",
                "lib_mod": "lib_mod",
                "lib_mrq": "lib_mrq_utac",
                "lib_mrq_doss": "lib_mrq_utac",
                "Marque": "lib_mrq_utac",
                "masse vide euro max (kg)": "masse_ordma_max",
                "masse vide euro min (kg)": "masse_ordma_min",
                "mod_utac": "lib_mod",
                "Mod√®le dossier": "lib_mod_doss",
                "Mod√®le UTAC": "lib_mod",
                "mrq_utac": "lib_mrq_utac",
                "NOX (g/km)": "nox",
                "Particules (g/km)": "ptcl",
                "puiss_admin_98": "puiss_admin",
                "puiss_heure": "puiss_h",
                "Puissance administrative": "puiss_admin",
                "Puissance maximale (kW)": "puiss_max",
                "typ_cbr": "typ_crb",
                "typ-crb" : "typ_crb",
                "Type Variante Version (TVV)": "tvv",
            }

            def standardize_column_names(df, column_mapping):
                df = df.copy()
                new_columns = df.columns.to_list()
                for keys, new_name in column_mapping.items():
                    if isinstance(keys, str):
                        keys = [keys]
                    for key in keys:
                        if key in new_columns:
                            new_columns[new_columns.index(key)] = new_name
                df.columns = new_columns
                return df

            mapping_results = {}
            for year, df_year in processed_dataframes.items():
                old_cols = set(df_year.columns)
                processed_dataframes[year] = standardize_column_names(df_year, column_mapping)
                new_cols = set(processed_dataframes[year].columns)
                
                renamed = len(old_cols.intersection(column_mapping.keys()))
                mapping_results[year] = {
                    'colonnes_avant': len(old_cols),
                    'colonnes_apr√®s': len(new_cols),
                    'colonnes_renomm√©es': renamed
                }
            
            df_mapping_results = pd.DataFrame(mapping_results).T
            safe_dataframe_display(df_mapping_results, "R√©sultats du mapping")
            
            st.success("‚úÖ Standardisation des noms de colonnes termin√©e")

            st.markdown("---")

            # √âTAPE 4: Classement alphab√©tique
            st.subheader(" √âtape 4 : Classement Alphab√©tique des Colonnes")
            
            def ordonner_colonnes_alphabetiquement(df):
                try:
                    colonnes_ordonnees = sorted(df.columns)
                    df_ordonne = df[colonnes_ordonnees]
                    return df_ordonne
                except Exception as e:
                    st.error(f"Erreur lors du classement des colonnes : {e}")
                    return df

            for year in processed_dataframes.keys():
                processed_dataframes[year] = ordonner_colonnes_alphabetiquement(processed_dataframes[year])
            
            st.success("‚úÖ Colonnes class√©es par ordre alphab√©tique pour tous les DataFrames")

            st.markdown("---")

            # √âTAPE 6: V√©rification de l'uniformit√©
            st.subheader(" √âtape 5 : V√©rification de l'Uniformit√© des Colonnes")
            
            st.markdown("#### ‚öñÔ∏è Analyse des diff√©rences entre DataFrames")
            
            def afficher_differences_streamlit(ensemble1, ensemble2, nom1, nom2):
                differences = ensemble1.symmetric_difference(ensemble2)
                if differences:
                    st.warning(f"‚ö†Ô∏è Diff√©rences entre {nom1} et {nom2} : {differences}")
                    return False
                else:
                    st.success(f"‚úÖ Aucune diff√©rence entre {nom1} et {nom2}")
                    return True

            sets_cols = {}
            for year, df_year in processed_dataframes.items():
                sets_cols[year] = set(df_year.columns)

            all_same = True
            years_list = sorted(list(processed_dataframes.keys()))
            
            if len(years_list) > 1:
                first_year = years_list[0]
                first_set = sets_cols[first_year]
                
                for i in range(1, len(years_list)):
                    current_year = years_list[i]
                    is_same = afficher_differences_streamlit(first_set, sets_cols[current_year],
                                                           first_year, current_year)
                    if not is_same:
                        all_same = False

            # Gestion sp√©cifique de puiss_h dans 2015
            if '2015' in processed_dataframes and 'puiss_h' in processed_dataframes['2015'].columns:
                puiss_h_in_others = any('puiss_h' in processed_dataframes[year].columns 
                                      for year in processed_dataframes if year != '2015')
                if not puiss_h_in_others:
                    processed_dataframes['2015'] = processed_dataframes['2015'].drop(columns=["puiss_h"], errors="ignore")
                    st.info("üóëÔ∏è Colonne 'puiss_h' supprim√©e de 2015 (absente des autres ann√©es)")

            # Re-v√©rification apr√®s ajustements
            st.markdown("#### üîÑ V√©rification finale apr√®s ajustements")
            sets_cols_final = {}
            for year, df_year in processed_dataframes.items():
                sets_cols_final[year] = set(df_year.columns)
            
            all_same_final = True
            
            if len(years_list) > 1:
                first_set_final = sets_cols_final[years_list[0]]
                for i in range(1, len(years_list)):
                    current_year = years_list[i]
                    is_same = afficher_differences_streamlit(first_set_final, sets_cols_final[current_year],
                                                           years_list[0], current_year)
                    if not is_same:
                        all_same_final = False

            if all_same_final:
                st.success("üéâ Tous les DataFrames ont maintenant les m√™mes colonnes ! Concat√©nation possible.")
            else:
                st.error("‚ùå Des diff√©rences persistent entre les DataFrames")

            st.markdown("---")

            # √âTAPE 7: D√©tection des colonnes dupliqu√©es
            st.subheader(" √âtape 6 : D√©tection des Colonnes Dupliqu√©es")
            
            st.markdown("#### üîÑ V√©rification et suppression des doublons de colonnes")
            
            def trouver_doublons(df, nom_df):
                colonnes_dupliquees = df.columns[df.columns.duplicated()].tolist()
                if colonnes_dupliquees:
                    st.warning(f"‚ö†Ô∏è Colonnes dupliqu√©es dans {nom_df} : {colonnes_dupliquees}")
                    processed_dataframes[nom_df.split('_')[1]] = df.loc[:, ~df.columns.duplicated()]
                    st.success(f"‚úÖ Colonnes dupliqu√©es supprim√©es de {nom_df}")
                    return len(colonnes_dupliquees)
                else:
                    st.success(f"‚úÖ Aucune colonne dupliqu√©e dans {nom_df}")
                    return 0

            total_duplicated_cols = 0
            for year, df_year in processed_dataframes.items():
                duplicated_count = trouver_doublons(df_year, f"df_{year}")
                total_duplicated_cols += duplicated_count

            if total_duplicated_cols == 0:
                st.success("üéâ Aucune colonne dupliqu√©e d√©tect√©e dans l'ensemble des DataFrames")

            st.markdown("---")

            # √âTAPE 8: Concat√©nation finale
            if all_same_final and len(processed_dataframes) > 0:
                st.subheader(" √âtape 7 : Cr√©ation du DataFrame final")
                
                df_final = pd.concat(processed_dataframes.values(), ignore_index=True)
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("üìä Lignes totales", f"{len(df_final):,}")
                
                with col2:
                    st.metric("üìã Colonnes", f"{len(df_final.columns):,}")
                
                with col3:
                    st.metric("üìÖ Ann√©es", f"{df_final['ann√©e'].nunique()}")
                
                with col4:
                    memory_usage = df_final.memory_usage(deep=True).sum() / 1024**2
                    st.metric("üíæ Taille", f"{memory_usage:.1f} MB")
                
                st.success("üéâ DataFrame final concat√©n√© cr√©√© avec succ√®s !")

            st.markdown("---")

            # √âTAPE 9: Analyse des valeurs manquantes
            if 'df_final' in locals():
                st.subheader(" √âtape 8 : Analyse des Valeurs Manquantes")
                
                st.markdown("#### üìä √âtat des valeurs manquantes")
                
                nan_counts = df_final.isnull().sum()
                nan_percentage = df_final.isnull().mean() * 100
                nan_summary = pd.DataFrame({
                    'Nombre de NaN': nan_counts, 
                    'Pourcentage de NaN': nan_percentage
                })
                
                safe_dataframe_display(nan_summary, "Analyse des valeurs manquantes")
                
                # Graphique Plotly au lieu de matplotlib
                if not nan_summary.empty:
                    nan_summary_filtered = nan_summary[nan_summary['Nombre de NaN'] > 0].sort_values(
                        by='Pourcentage de NaN', ascending=False)
                    
                    if len(nan_summary_filtered) > 0:
                        fig_nan = px.bar(
                            x=nan_summary_filtered.index,
                            y=nan_summary_filtered['Pourcentage de NaN'],
                            title="Pourcentage de valeurs manquantes par colonne",
                            labels={'x': 'Colonnes', 'y': 'Pourcentage de NaN (%)'},
                            color=nan_summary_filtered['Pourcentage de NaN'],
                            color_continuous_scale='Reds'
                        )
                        fig_nan.update_layout(
                            xaxis_tickangle=-45,
                            height=500
                        )
                        st.plotly_chart(fig_nan, use_container_width=True)

            st.markdown("---")

            # √âTAPE 10: Conversion des types de donn√©es
            if 'df_final' in locals():
                st.subheader(" √âtape 9 : Conversion des Types de Donn√©es vers types num√©riques")
                
                cols_numeriques = ["puiss_max", "conso_urb", "conso_exurb", "conso_mixte", "co_typ_1", "nox", "hcnox", "ptcl", "co2"]

                def convert_numeric(value):
                    if isinstance(value, str):
                        value = value.replace(",", ".")
                        try:
                            return float(value)
                        except ValueError:
                            return np.nan
                    return value

                conversion_results = {}
                for col in cols_numeriques:
                    if col in df_final.columns:
                        before_type = str(df_final[col].dtype)
                        before_nulls = df_final[col].isnull().sum()
                        
                        df_final[col] = df_final[col].apply(convert_numeric)
                        
                        after_type = str(df_final[col].dtype)
                        after_nulls = df_final[col].isnull().sum()
                        
                        conversion_results[col] = {
                            'Type avant': before_type,
                            'Type apr√®s': after_type,
                            'NaN avant': before_nulls,
                            'NaN apr√®s': after_nulls,
                            'Nouveaux NaN': after_nulls - before_nulls
                        }

                if conversion_results:
                    df_conversion = pd.DataFrame(conversion_results).T
                    safe_dataframe_display(df_conversion, "R√©sultats des conversions")
                    st.success("‚úÖ Conversion des colonnes num√©riques termin√©e")

            st.markdown("---")

            # √âTAPE 11: Nettoyage des donn√©es cat√©gorielles
            if 'df_final' in locals():
                st.subheader(" √âtape 10 : Standardisation des variables cat√©gorielles")
                
                # Nettoyer 'gamme'
                if "gamme" in df_final.columns:
                    st.markdown("##### Nettoyage de la colonne 'gamme'")
                    before_unique = df_final["gamme"].nunique()
                    
                    df_final["gamme"] = df_final["gamme"].str.upper().str.strip()

                    corrections_gamme = {
                        "MOY-INF": "MOY-INFER",
                        "MOY-INFERIEURE": "MOY-INFER"
                    }
                    df_final["gamme"] = df_final["gamme"].replace(corrections_gamme)
                    
                    after_unique = df_final["gamme"].nunique()
                    st.success(f"‚úÖ 'gamme' nettoy√©e : {before_unique} ‚Üí {after_unique} valeurs uniques")
                    
                    gamme_counts = df_final["gamme"].value_counts().head()
                    safe_dataframe_display(gamme_counts.to_frame(), "Top valeurs 'gamme'")

                # Nettoyer 'Carrosserie'
                if "Carrosserie" in df_final.columns:
                    st.markdown("##### Nettoyage de la colonne 'Carrosserie'")
                    before_unique = df_final["Carrosserie"].nunique()
                    
                    df_final["Carrosserie"] = df_final["Carrosserie"].str.upper().str.strip()

                    corrections_carrosserie = {
                        "COMBISPCACE": "COMBISPACE"
                    }
                    df_final["Carrosserie"] = df_final["Carrosserie"].replace(corrections_carrosserie)
                    
                    after_unique = df_final["Carrosserie"].nunique()
                    st.success(f"‚úÖ 'Carrosserie' nettoy√©e : {before_unique} ‚Üí {after_unique} valeurs uniques")

                # Nettoyer 'typ_crb'
                if "typ_crb" in df_final.columns:
                    st.markdown("##### Nettoyage de la colonne 'typ_crb'")
                    before_unique = df_final["typ_crb"].nunique()
                    
                    df_final["typ_crb"] = df_final["typ_crb"].str.upper().str.strip()

                    corrections_typ_crb = {
                        "GO ": "GO", "ES ": "ES", "EH ": "EH", "GH ": "GH", "GN ": "GN", "EE ": "EE",
                        "ES/GN": "GN/ES", "GP/ES": "ES/GP"
                    }
                    df_final["typ_crb"] = df_final["typ_crb"].replace(corrections_typ_crb)
                    
                    after_unique = df_final["typ_crb"].nunique()
                    st.success(f"‚úÖ 'typ_crb' nettoy√©e : {before_unique} ‚Üí {after_unique} valeurs uniques")
                    
                    typ_crb_counts = df_final["typ_crb"].value_counts().head()
                    safe_dataframe_display(typ_crb_counts.to_frame(), "Distribution 'typ_crb'")

            st.markdown("---")

            # √âTAPE 12: Imputation intelligente
            if 'df_final' in locals():
                st.subheader(" √âtape 11 : Imputation Intelligente des Valeurs Manquantes")
                
                # Imputation sp√©cifique pour 2015
                st.markdown("#### üìÖ Imputation sp√©ciale pour l'ann√©e 2015")
                
                df_train = df_final[df_final["ann√©e"] < 2015]

                mapping_carrosserie = df_train.groupby("gamme")["Carrosserie"].agg(lambda x: x.value_counts().idxmax())
                mapping_gamme = df_train.groupby("Carrosserie")["gamme"].agg(lambda x: x.value_counts().idxmax())

                df_2015 = df_final[df_final["ann√©e"] == 2015]

                if not df_train.empty and not df_2015.empty:
                    gamme_sample = df_train["gamme"].dropna().sample(n=len(df_2015), replace=True, random_state=42).values

                    df_final.loc[df_final["ann√©e"] == 2015, "gamme"] = gamme_sample
                    df_final.loc[df_final["ann√©e"] == 2015, "Carrosserie"] = df_final.loc[df_final["ann√©e"] == 2015, "gamme"].map(mapping_carrosserie)
                    
                    st.success("‚úÖ Imputation 2015 : gamme et Carrosserie bas√©es sur la distribution historique")

                # Imputation par groupe pour variables num√©riques
                st.markdown("#### üî¢ Imputation des variables num√©riques par groupe")
                
                # hcnox
                if "hcnox" in df_final.columns:
                    before_nulls = df_final["hcnox"].isna().sum()
                    df_final["hcnox"] = df_final.groupby("typ_crb")["hcnox"].transform(lambda x: x.fillna(x.median()))
                    df_final["hcnox"].fillna(df_final["hcnox"].median(), inplace=True)
                    after_nulls = df_final["hcnox"].isna().sum()
                    st.success(f"‚úÖ hcnox: {before_nulls} ‚Üí {after_nulls} NaN (imputation par groupe typ_crb)")

                # ptcl
                if "ptcl" in df_final.columns:
                    before_nulls = df_final["ptcl"].isna().sum()
                    df_final["ptcl"] = df_final.groupby("typ_crb")["ptcl"].transform(lambda x: x.fillna(x.median()))
                    df_final["ptcl"].fillna(df_final["ptcl"].median(), inplace=True)
                    after_nulls = df_final["ptcl"].isna().sum()
                    st.success(f"‚úÖ ptcl: {before_nulls} ‚Üí {after_nulls} NaN (imputation par groupe typ_crb)")

                # Imputation finale
                st.markdown("#### üéØ Imputation finale")
                
                cols_numeriques = ["co2","co_typ_1","conso_exurb","conso_mixte","conso_urb","nox","puiss_max"]
                cols_categorielles = ["champ_v9"] 

                numeric_imputed = 0
                for col in cols_numeriques:
                    if col in df_final.columns and df_final[col].isnull().any():
                        before_nulls = df_final[col].isnull().sum()
                        df_final[col] = df_final[col].fillna(df_final[col].mean())
                        numeric_imputed += before_nulls

                categorical_imputed = 0
                for col in cols_categorielles:
                    if col in df_final.columns and df_final[col].isnull().any():
                        before_nulls = df_final[col].isnull().sum()
                        mode_val = df_final[col].mode()
                        if not mode_val.empty:
                            df_final[col] = df_final[col].fillna(mode_val[0])
                            categorical_imputed += before_nulls

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("üî¢ NaN num√©riques imput√©s", numeric_imputed)
                with col2:
                    st.metric("üè∑Ô∏è NaN cat√©goriels imput√©s", categorical_imputed)
                with col3:
                    remaining_nulls = df_final.isnull().sum().sum()
                    st.metric("üéØ NaN restants", remaining_nulls)

                if remaining_nulls == 0:
                    st.success("üéâ Toutes les valeurs manquantes ont √©t√© imput√©es avec succ√®s !")

            st.markdown("---")

            # √âTAPE 13: Nettoyage lib_mrq_utac
            if 'df_final' in locals():
                st.subheader(" √âtape 12 : Nettoyage des Marques (lib_mrq_utac)")
                
                if "lib_mrq_utac" in df_final.columns:
                    before_unique = df_final["lib_mrq_utac"].nunique()
                    
                    df_final["lib_mrq_utac"] = df_final["lib_mrq_utac"].str.strip()
                    replace_dict = {
                        "BMW I": "BMW",
                        "BMW ": "BMW",
                        "MERCEDES AMG": "MERCEDES",
                        "MERCEDES BENZ": "MERCEDES",
                        "MERCEDES-BENZ": "MERCEDES",
                        "MERCEDES ": "MERCEDES",
                        "ALFA-ROMEO": "ALFA ROMEO",
                        "ROLLS-ROYCE" : "ROLLS ROYCE",
                        "LAND ROVER": "JAGUAR LAND ROVER LIMITED",
                        "RENAULT TECH": "RENAULT",
                        "RENAULT ": "RENAULT",
                        "FORD-CNG-TECHNIK" : "FORD",
                        "VOLKSWAGEN-TECHNIK" : "VOLKSWAGEN",
                    }

                    df_final["lib_mrq_utac"] = df_final["lib_mrq_utac"].replace(replace_dict)
                    
                    after_unique = df_final["lib_mrq_utac"].nunique()
                    st.success(f"‚úÖ lib_mrq_utac nettoy√©e : {before_unique} ‚Üí {after_unique} marques uniques")

            st.markdown("---")

            # √âTAPE 14: Suppression des colonnes administratives
            if 'df_final' in locals():
                st.subheader(" √âtape 13 : Suppression des Colonnes Administratives")
                
                df_final_ml = df_final.copy()
                
                to_remove = [
                    "champ_v9", "cnit", "date_maj", "dscom", "hc", 
                    "hybride", "lib_mod", "lib_mod_doss", "tvv"
                ]

                cols_before = df_final.columns.tolist()
                df_final_ml.drop(columns=to_remove, inplace=True, errors="ignore")
                cols_after = df_final_ml.columns.tolist()
                removed = [c for c in cols_before if c not in cols_after]
                
                st.success(f"‚úÖ Colonnes administratives supprim√©es : {', '.join(removed)}")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("üìä Colonnes avant", len(cols_before))
                with col2:
                    st.metric("üóëÔ∏è Colonnes supprim√©es", len(removed))
                with col3:
                    st.metric("‚úÖ Colonnes finales", len(cols_after))

            st.markdown("---")

            # avec sauvegarde
            if 'df_final' in locals() and 'df_final_ml' in locals():
                st.subheader(" √âtape 14 : Sauvegarde du dataframe post preprocessing")
                
                st.session_state['df_final'] = df_final
                st.session_state['df_final_ml'] = df_final_ml
                st.session_state['preprocessing_completed'] = True
                
                st.success("‚úÖ Preprocessing termin√© et stock√© pour les autres onglets !")
                
                st.markdown("---")
                st.markdown("""
                ### üéØ Points Cl√©s du Preprocessing R√©alis√©
                
                **‚úÖ Qualit√© des donn√©es :**
                - Suppression de tous les doublons
                - Harmonisation compl√®te des colonnes entre ann√©es
                - Imputation intelligente de toutes les valeurs manquantes
                - Suppression des colonnes non pertinentes (phase 1)
                
                **‚úÖ Standardisation :**
                - Noms de colonnes uniformis√©s (gr√¢ce √† un mapping)
                - Variables cat√©gorielles nettoy√©es et standardis√©es
                - Types de donn√©es convertis correctement
                """)
                
        # Informations sur les prochaines √©tapes
        st.markdown("---")
        st.info("""
        **üöÄ Prochaines √©tapes :**
        
        Naviguez vers l'onglet **Visualisation** pour :
        - Explorer les corr√©lations entre variables
        - Analyser la relation puissance-CO2
        - √âtudier l'impact du type de carburant
        - Analyser les outliers
        """)

    # ----------------------------------------------------------------
    # Onglet Visualisation des Donn√©es (index 2)
    # ----------------------------------------------------------------
    with tabs[2]:
        st.markdown("Data Visualisation pour la compr√©hension des relations entre les donn√©es et prise de d√©cision quant au Feature Engineering")
        
        if 'df_final' in st.session_state:
            df_final = st.session_state['df_final']
        else:
            df_final = None
        
        viz_tabs = st.tabs(["Aper√ßu Rapide du Projet", "Analyse de Corr√©lation", "Analyse des Outliers"])

        # Onglet 1 : Aper√ßu Rapide du Projet
        with viz_tabs[0]:
            if df_final is None or df_final.empty:
                st.info("Ex√©cutez d'abord l'onglet 'Preprocessing' pour construire df_final.")
            else:
                # 1) Relation puissance administrative et CO2 - MODIFICATION: Plotly
                st.markdown("### Relation entre Puissance Administrative et CO2")
                
                if "puiss_admin" in df_final.columns and "co2" in df_final.columns:
                    # Cr√©er le graphique avec Plotly
                    fig_power = px.scatter(
                        df_final,
                        x="puiss_admin",
                        y="co2",
                        title="Relation entre la puissance administrative et le CO2",
                        labels={"puiss_admin": "Puissance Administrative", "co2": "CO2 (g/km)"},
                        opacity=0.6,
                        color_discrete_sequence=['steelblue']
                    )
                    
                    fig_power.update_layout(
                        height=500,
                        title_x=0.5
                    )
                    
                    st.plotly_chart(fig_power, use_container_width=True)
                    
                    corr, p_value = pearsonr(df_final["puiss_admin"].dropna(), 
                                        df_final["co2"].dropna())
                    
                    st.markdown(f"""
                    **üìà Analyse du Pearson :**
                    - **Corr√©lation de Pearson** : **{corr:.3f}** (p-value: {p_value:.2e})
                    - **Interpr√©tation** : {'Forte' if abs(corr) > 0.7 else 'Mod√©r√©e' if abs(corr) > 0.5 else 'Faible'} corr√©lation positive
                    - **Tendance claire** : Les √©missions de CO2 augmentent avec la puissance administrative
                    - **Logique physique** : V√©hicules plus puissants = moteurs plus grands = plus de consommation = plus de pollution
                    """)
                
                st.markdown("---")
                
                # 2) Impact du type de carburant sur les √©missions
                st.markdown("### Impact du Type de Carburant sur les √âmissions CO2")
                
                if "typ_crb" in df_final.columns:
                    fig_fuel_box = px.box(
                        df_final,
                        x="typ_crb",
                        y="co2",
                        title="Distribution des √©missions CO2 par type de carburant",
                        labels={"typ_crb": "Type de carburant", "co2": "CO2 (g/km)"},
                        color="typ_crb",
                        color_discrete_sequence=px.colors.qualitative.Set2
                    )
                    fig_fuel_box.update_layout(
                        xaxis_title="Type de carburant",
                        yaxis_title="CO2 (g/km)",
                        title_x=0.5,
                        showlegend=False
                    )
                    st.plotly_chart(fig_fuel_box, use_container_width=True)
                    
                                        
                    try:
                        groupes = [df_final[df_final["typ_crb"] == cat]["co2"].dropna() 
                                for cat in df_final["typ_crb"].unique() if not pd.isna(cat)]
                        anova_result = f_oneway(*groupes)
                        
                        st.markdown(f"""
                        **üìä Analyse de l'ANOVA :**
                        - **F-statistic** : **{anova_result.statistic:.2f}**
                        - **P-value** : **{anova_result.pvalue:.2e}**
                        - **Conclusion** : {'Diff√©rence tr√®s significative' if anova_result.pvalue < 0.001 else 'Diff√©rence significative' if anova_result.pvalue < 0.05 else 'Pas de diff√©rence significative'} entre les types de carburant
                        """)
                        
                    except Exception as e:
                        st.warning(f"Impossible de calculer l'ANOVA : {e}")

        # Onglet 2 : Analyse de Corr√©lation
        with viz_tabs[1]:
            if df_final is None or df_final.empty:
                st.info("Ex√©cutez d'abord l'onglet 'Preprocessing' pour construire df_final avant d'analyser les corr√©lations.")
            else:
                # 1) Heatmap des corr√©lations num√©riques
                st.markdown("### Corr√©lation des Variables Num√©riques")
                df_num = df_final.select_dtypes(include=np.number).dropna(axis=1, how='all')

                if df_num.shape[1] > 1:
                    corr_matrix = df_num.corr()
                    fig6, ax6 = plt.subplots(figsize=(12, 10))
                    sns.heatmap(corr_matrix,
                                annot=True, fmt=".2f",
                                cmap="coolwarm", linewidths=0.5,
                                ax=ax6, center=0)
                    ax6.set_title("Heatmap des corr√©lations num√©riques")
                    plt.tight_layout()
                    st.pyplot(fig6)
                    
                    # Analyse de la multicolin√©arit√©
                                        
                    high_corr_pairs = []
                    for i in range(len(corr_matrix.columns)):
                        for j in range(i+1, len(corr_matrix.columns)):
                            corr_val = corr_matrix.iloc[i, j]
                            if abs(corr_val) > 0.8:
                                high_corr_pairs.append({
                                    'Variable 1': corr_matrix.columns[i],
                                    'Variable 2': corr_matrix.columns[j],
                                    'Corr√©lation': corr_val
                                })
                    
                    if high_corr_pairs:
                        df_high_corr = pd.DataFrame(high_corr_pairs)
                        df_high_corr = df_high_corr.sort_values('Corr√©lation', key=abs, ascending=False)
                        
                        st.warning("**‚ö†Ô∏èImpact sur la mod√©lisation: risque de multicolin√©arit√© d√©tect√© !**")
                        safe_dataframe_display(df_high_corr, "Corr√©lations √©lev√©es (|r| > 0.8)")
                        
                        st.markdown("""
                        **üìã Interpr√©tation :**
                        - **Variables de consommation** (`conso_urb`, `conso_exurb`, `conso_mixte`) : tr√®s corr√©l√©es entre elles et avec CO2
                        - **Variables de masse** (`masse_ordma_min`, `masse_ordma_max`) : corr√©lation tr√®s √©lev√©e (redondance)
                        - **Variables de puissance** (`puiss_admin`, `puiss_max`) : corr√©lation √©lev√©e
                        
                        ‚û°Ô∏è **Solution** : supprimer ces variables redondantes dans l'√©tape Feature Engineering
                        """)
                        
                        multicollinear_vars = ["conso_exurb", "conso_mixte", "conso_urb", 
                                             "masse_ordma_max", "masse_ordma_min", 
                                             "puiss_admin", "puiss_max"]
                        st.session_state['multicollinear_vars'] = multicollinear_vars
                        
                    else:
                        st.success("‚úÖ Aucune corr√©lation probl√©matique d√©tect√©e (seuil |r| > 0.8)")
                    
                else:
                    st.info("Pas assez de colonnes num√©riques (au moins 2) disponibles apr√®s traitement pour g√©n√©rer une heatmap de corr√©lation.")

                # 2) Analyse des corr√©lations entre variables cat√©gorielles avec Cramer's V
                st.markdown('---')
                st.markdown("### Corr√©lations entre Variables Cat√©gorielles (Cramer's V)")
                categories = ['Carrosserie', 'gamme', 'lib_mod_doss', 'lib_mrq_utac', 'typ_boite_nb_rapp', 'typ_crb']
                categories_available = [col for col in categories if col in df_final.columns]
                if len(categories_available) >= 2:
                    cramer_matrix = pd.DataFrame(np.zeros((len(categories_available), len(categories_available))), index=categories_available, columns=categories_available)
                    for col1 in categories_available:
                        for col2 in categories_available:
                            if col1 != col2:
                                try:
                                    cramer_matrix.loc[col1, col2] = cramers_v(df_final[col1], df_final[col2])
                                except:
                                    cramer_matrix.loc[col1, col2] = 0
                            else:
                                cramer_matrix.loc[col1, col2] = 1
                    cramer_matrix = cramer_matrix.astype(float)
                    (fig_cramer, ax_cramer) = plt.subplots(figsize=(10, 8))
                    sns.heatmap(cramer_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', ax=ax_cramer)
                    ax_cramer.set_title("Heatmap de Cramer's V (corr√©lation entre variables cat√©gorielles)")
                    plt.tight_layout()
                    st.pyplot(fig_cramer)
                    
                    st.warning("‚ö†Ô∏è **Impact sur la mod√©lisation : associations fortes r√©v√©l√©es par le Cramer's V qui s'expliquent par la logique m√©tier du secteur automobile.**")
                    
                    strong_associations = []
                    for i in range(len(cramer_matrix.columns)):
                        for j in range(i + 1, len(cramer_matrix.columns)):
                            cramer_val = cramer_matrix.iloc[i, j]
                            if cramer_val > 0.3:
                                strong_associations.append({'Variable 1': cramer_matrix.columns[i], 'Variable 2': cramer_matrix.columns[j], "Cramer's V": cramer_val})
                    if strong_associations:
                        df_strong_assoc = pd.DataFrame(strong_associations)
                        df_strong_assoc = df_strong_assoc.sort_values("Cramer's V", ascending=False)
                        safe_dataframe_display(df_strong_assoc, 'Associations cat√©gorielles fortes')
                        
                        st.markdown('**üìã Interpr√©tation :**')
                        st.markdown("""
                                                
                        - **Marque ‚Üî Gamme** : chaque constructeur a sa propre segmentation (premium, g√©n√©raliste, etc.)
                        - **Carrosserie ‚Üî Gamme** : les berlines sont souvent en gamme sup√©rieure, les citadines en gamme inf√©rieure  
                        - **Type de carburant ‚Üî Gamme** : les v√©hicules haut de gamme adoptent plus facilement l'hybride/√©lectrique
                        - **Marque ‚Üî Type de bo√Æte** : certains constructeurs privil√©gient l'automatique (premium) ou le manuel (g√©n√©raliste)
                        """)
                        
                        st.markdown("**‚û°Ô∏è Solution :** mise en place d'une strat√©gie d'encodage diff√©renci√©e par variable dans l'√©tape Feature Engineering :")
                        st.markdown("""
                                                
                        - `typ_crb` : regroupement en 3 cat√©gories principales (`GO`/`ES`/`Autres`) puis OneHot
                        - `lib_mrq_utac` : encodage fr√©quentiel (trop de modalit√©s pour OneHot)  
                        - `Carrosserie`/`gamme` : OneHot avec drop='first' pour √©viter la redondance
                        - `typ_boite_nb_rapp` : encodage fr√©quentiel (modalit√©s nombreuses et vari√©es)
                      
                                    
                        """)

                        st.markdown('---')
                        st.markdown('### Conclusions g√©n√©rales')
                        st.success("""
                        
                        - **Variables num√©riques :** la multicolin√©arit√© d√©tect√©e est probl√©matique et sera trait√©e dans le Feature Engineering pour ne pas fausser les r√©sultats de nos mod√®les de r√©gression.

                        - **Variables cat√©gorielles :** les associations d√©tect√©es sont normales et attendues dans le contexte automobile. Notre strat√©gie d'encodage diff√©renci√© permet de les exploiter efficacement sans cr√©er de probl√®mes de dimensionnalit√©.
                        """)

                else:
                    st.info(f"Pas assez de variables cat√©gorielles disponibles pour l'analyse Cramer's V. Variables trouv√©es : {categories_available}")

        # Onglet 3 : Analyse des Outliers
        with viz_tabs[2]:
                            
            if df_final is None or df_final.empty:
                st.info("Ex√©cutez d'abord l'onglet 'Preprocessing' pour construire df_final avant d'analyser les outliers.")
            else:
                
                st.markdown("#### Identification des valeurs aberrantes")
                
                numerical_cols = df_final.select_dtypes(include=['float64', 'int64']).columns

                Q1 = df_final[numerical_cols].quantile(0.25)
                Q3 = df_final[numerical_cols].quantile(0.75)
                IQR = Q3 - Q1

                outliers = ((df_final[numerical_cols] < (Q1 - 1.5 * IQR)) | (df_final[numerical_cols] > (Q3 + 1.5 * IQR)))

                outliers_count = outliers.sum()
                
                df_outliers = pd.DataFrame({
                    'Colonne': outliers_count.index,
                    'Nombre d\'outliers': outliers_count.values,
                    'Pourcentage': (outliers_count.values / len(df_final) * 100).round(2)
                })
                
                safe_dataframe_display(df_outliers, "Analyse des outliers")
                
                fig_outliers = px.bar(
                    df_outliers.sort_values('Pourcentage', ascending=False).head(10),
                    x='Pourcentage',
                    y='Colonne',
                    orientation='h',
                    title="Top 10 des variables avec le plus d'outliers",
                    labels={'Pourcentage': 'Pourcentage d\'outliers (%)', 'Colonne': 'Variables'},
                    color='Pourcentage',
                    color_continuous_scale='Reds'
                )
                fig_outliers.update_layout(height=500)
                st.plotly_chart(fig_outliers, use_container_width=True)
                
                if "puiss_admin" in df_final.columns:
                    high_power_cars = df_final[df_final["puiss_admin"] > 60]
                    if len(high_power_cars) > 0 and "lib_mrq_utac" in df_final.columns:
                        brand_counts = high_power_cars["lib_mrq_utac"].value_counts().head(10)
                                
                        fig_brands = px.bar(
                            x=brand_counts.values, 
                            y=brand_counts.index, 
                            orientation='h',
                            title="Top 10 des marques avec puiss_admin > 60",
                            labels={'x': 'Nombre de v√©hicules', 'y': 'Marque (lib_mrq_utac)'},
                            color=brand_counts.values,
                            color_continuous_scale='viridis'
                            )
                        fig_brands.update_layout(
                            height=500,
                            showlegend=False,
                            coloraxis_showscale=False
                            )
                        st.plotly_chart(fig_brands, use_container_width=True)
                
                # Boxplots pour visualiser les outliers
                st.markdown("#### Boxplots des variables num√©riques")
                
                numerical_cols_for_boxplot = df_final.select_dtypes(include=["int64", "float64"]).columns
                
                if len(numerical_cols_for_boxplot) > 0:
                                        
                    n_cols = min(3, len(numerical_cols_for_boxplot))
                    n_rows = (len(numerical_cols_for_boxplot) + n_cols - 1) // n_cols
                    
                    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
                    if n_rows == 1:
                        axes = [axes] if n_cols == 1 else axes
                    else:
                        axes = axes.flatten()
                    
                    for i, col in enumerate(numerical_cols_for_boxplot):
                        if i < len(axes):
                            sns.boxplot(x=df_final[col], ax=axes[i])
                            axes[i].set_title(f"Boxplot de {col}")
                    
                    for i in range(len(numerical_cols_for_boxplot), len(axes)):
                        axes[i].set_visible(False)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                else:
                    st.info("Aucune colonne num√©rique trouv√©e pour l'analyse en boxplot.")

                # Conclusions
                st.markdown("#### Conclusions g√©n√©rales")
                
                st.markdown("""
                **Observations principales :**
                - Certaines variables pr√©sentent des outliers significatifs
                - Ce outliers s'expliquent (exemple : v√©hicules de forte puissance sont souvent des v√©hicules de luxe ou sportifs)
                - Les outliers peuvent repr√©senter des segments sp√©cifiques du march√© automobile
                
                **Strat√©gies de traitement :**
                - Certaines colonnes seront supprim√©es car d√©j√† identifi√©es dans l'analyse des corr√©lations (`conso_urb`,`conso_mixte`,`conso_exurb`, etc...)
                - Pas d'actions sp√©cifiques pr√©vues pour les autres hormis des op√©rations d'encodage
                
                **Impact sur la mod√©lisation :**
                - Cela nous a aider √† identifier les mod√®les √† utiliser qui g√®rent mieux les outliers (Random Forest, XGBoost)
                """)


            st.markdown("---")
            st.info("""
            **üöÄ Prochaines √©tapes :**
            
            Naviguez vers l'onglet **Feature Engineering** pour :
            - Supprimer les variables multicolin√©aires identifi√©es
            - S√©parer les donn√©es d'entra√Ænement et de test
            - Encoder les variables cat√©gorielles
            - Standardiser les variables num√©riques
            - Pr√©parer les donn√©es finales pour la mod√©lisation
            """)

    # ----------------------------------------------------------------
    # Onglet Feature Engineering (index 3)
    # ----------------------------------------------------------------
    with tabs[3]:
        st.markdown("Am√©lioration et r√©alisation de la phase 2 de notre dataframe cible")
        
        if 'df_final_ml' in st.session_state:
            df_final_ml = st.session_state['df_final_ml']
        else:
            df_final_ml = None

        if df_final_ml is None or df_final_ml.empty:
            st.info("Veuillez d'abord ex√©cuter l'onglet 'Preprocessing' pour obtenir df_final_ml.")
        else:
            # √âTAPE 1: Suppression des variables multicolin√©aires
            st.subheader(" √âtape 1 : Suppression des Variables Multicolin√©aires")
            
            multicollinear_vars = ["conso_exurb", "conso_mixte", "conso_urb", 
                                 "masse_ordma_max", "masse_ordma_min", 
                                 "puiss_admin", "puiss_max"]
            
            existing_multicol_vars = [var for var in multicollinear_vars if var in df_final_ml.columns]
            
            if existing_multicol_vars:
                if len(existing_multicol_vars) > 1:
                    corr_before = df_final_ml[existing_multicol_vars + ['co2']].corr()['co2'].drop('co2')
                    
                    df_corr_display = pd.DataFrame({
                        'Variable': corr_before.index,
                        'Corr√©lation avec CO2': corr_before.values.round(3),
                        'Statut': ['üóëÔ∏è √Ä supprimer'] * len(corr_before)
                    })
                    
                    safe_dataframe_display(df_corr_display, "Variables multicolin√©aires")
                
                cols_before_multicol = df_final_ml.columns.tolist()
                df_final_ml = df_final_ml.drop(columns=existing_multicol_vars, errors='ignore')
                cols_after_multicol = df_final_ml.columns.tolist()
                
                removed_multicol = [c for c in cols_before_multicol if c not in cols_after_multicol]
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("üìä Colonnes avant", len(cols_before_multicol))
                with col2:
                    st.metric("üóëÔ∏è Variables supprim√©es", len(removed_multicol))
                with col3:
                    st.metric("‚úÖ Colonnes restantes", len(cols_after_multicol))
                
                st.success(f"‚úÖ Variables multicolin√©aires supprim√©es : {', '.join(removed_multicol)}")
                
            else:
                st.info("‚ÑπÔ∏è Aucune variable multicolin√©aire trouv√©e dans df_final_ml")

            st.markdown("---")

            # √âTAPE 2: √âtat Initial des donn√©es apr√®s suppression multicolin√©arit√©
            st.subheader(" √âtape 2 : √âtat des Donn√©es apr√®s Nettoyage")
            
            st.markdown("#### üìã R√©capitulatif des donn√©es nettoy√©es")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("üìä Nombre de lignes", f"{df_final_ml.shape[0]:,}")
            
            with col2:
                st.metric("üìã Nombre de colonnes", f"{df_final_ml.shape[1]:,}")
            
            with col3:
                st.metric("üéØ Variable cible", "co2")
            
            colonnes_numeriques = df_final_ml.select_dtypes(include=np.number).columns.tolist()
            colonnes_categorielles = df_final_ml.select_dtypes(include='object').columns.tolist()
            
            toutes_colonnes = df_final_ml.columns.tolist()
            types_colonnes = []
            for col in toutes_colonnes:
                if col in colonnes_numeriques:
                    types_colonnes.append('Num√©rique')
                elif col in colonnes_categorielles:
                    types_colonnes.append('Cat√©gorielle')
                else:
                    types_colonnes.append('Autre')
            
            df_recap = pd.DataFrame({
                'Colonne': toutes_colonnes,
                'Type': types_colonnes,
                'Valeurs Uniques': [df_final_ml[col].nunique() for col in toutes_colonnes],
                'Valeurs Manquantes': [df_final_ml[col].isnull().sum() for col in toutes_colonnes],
                '% Manquantes': [round(df_final_ml[col].isnull().sum() / len(df_final_ml) * 100, 2) for col in toutes_colonnes]
            })
            
            safe_dataframe_display(df_recap, "√âtat final des donn√©es pour ML")

            st.markdown("---")
            
            # √âTAPE 3: S√©paration du jeu d'entra√Ænement et de test
            st.subheader(" √âtape 3 : S√©paration Train/Test AVANT Encodage")
            
            if 'co2' not in df_final_ml.columns:
                st.error("Colonne 'co2' non trouv√©e dans df_final_ml. Impossible de proc√©der au Feature Engineering.")
            else:
                X = df_final_ml.drop("co2", axis=1)
                y = df_final_ml["co2"]

                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("üèãÔ∏è X_train", f"{X_train.shape[0]:,} √ó {X_train.shape[1]:,}")
                
                with col2:
                    st.metric("üß™ X_test", f"{X_test.shape[0]:,} √ó {X_test.shape[1]:,}")
                
                with col3:
                    st.metric("üéØ y_train", f"{y_train.shape[0]:,}")
                
                with col4:
                    st.metric("üéØ y_test", f"{y_test.shape[0]:,}")
                
                st.success("‚úÖ Division train/test effectu√©e (80/20) avec random_state=42")

            st.markdown("---")
            
            # √âTAPE 4: Encodage des variables
            st.subheader(" √âtape 4 : Encodage des Variables Cat√©gorielles")
            
            X_train = X_train.copy()
            X_test = X_test.copy()
            
            st.markdown("#### üîÑ OneHotEncoder pour Carrosserie et gamme")
            
            ohe_cols = ["Carrosserie", "gamme"]
            ohe_cols = [col for col in ohe_cols if col in X_train.columns]
            
            if ohe_cols:
                ohe = OneHotEncoder(drop="first", sparse_output=False)
                
                X_train_ohe = ohe.fit_transform(X_train[ohe_cols])
                X_test_ohe = ohe.transform(X_test[ohe_cols])
                
                X_train_ohe = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names_out(ohe_cols))
                X_test_ohe = pd.DataFrame(X_test_ohe, columns=ohe.get_feature_names_out(ohe_cols))
                
                X_train = X_train.drop(columns=ohe_cols).reset_index(drop=True)
                X_test = X_test.drop(columns=ohe_cols).reset_index(drop=True)
                X_train = pd.concat([X_train, X_train_ohe], axis=1)
                X_test = pd.concat([X_test, X_test_ohe], axis=1)
                
                st.success(f"‚úÖ OneHotEncoder appliqu√© √† : {', '.join(ohe_cols)}")
                
            else:
                st.warning("‚ö†Ô∏è Aucune colonne trouv√©e pour OneHotEncoder (Carrosserie, gamme)")

            st.markdown("#### ‚õΩ Encodage sp√©cial pour typ_crb")
            
            if 'typ_crb' in X_train.columns:
                def regrouper_carburant(carb):
                    if carb == "GO":
                        return "GO"
                    elif carb == "ES":
                        return "ES"
                    else:
                        return "Autres"

                X_train["typ_crb_grp"] = X_train["typ_crb"].apply(regrouper_carburant)
                X_test["typ_crb_grp"] = X_test["typ_crb"].apply(regrouper_carburant)

                distrib_carb = X_train["typ_crb_grp"].value_counts()
                
                ohe_typ_crb = OneHotEncoder(drop=None, sparse_output=False)

                X_train_crb = ohe_typ_crb.fit_transform(X_train[["typ_crb_grp"]])
                X_test_crb = ohe_typ_crb.transform(X_test[["typ_crb_grp"]])

                X_train_crb = pd.DataFrame(X_train_crb, columns=ohe_typ_crb.get_feature_names_out(["typ_crb_grp"]))
                X_test_crb = pd.DataFrame(X_test_crb, columns=ohe_typ_crb.get_feature_names_out(["typ_crb_grp"]))

                X_train = X_train.drop(columns=["typ_crb", "typ_crb_grp"]).reset_index(drop=True)
                X_test = X_test.drop(columns=["typ_crb", "typ_crb_grp"]).reset_index(drop=True)

                X_train = pd.concat([X_train, X_train_crb], axis=1)
                X_test = pd.concat([X_test, X_test_crb], axis=1)
                
                st.success("‚úÖ typ_crb regroup√© (GO/ES/Autres) et encod√© avec succ√®s")
                
            else:
                st.warning("‚ö†Ô∏è Colonne 'typ_crb' non trouv√©e pour l'encodage")

            st.markdown("#### üìä Encodage fr√©quentiel")
            
            freq_cols = ["lib_mrq_utac", "typ_boite_nb_rapp"]
            freq_cols = [col for col in freq_cols if col in X_train.columns]
            
            if freq_cols:
                freq_maps = {col: X_train[col].value_counts(normalize=True) for col in freq_cols}

                for col in freq_cols:
                    before_unique = X_train[col].nunique()
                    X_train[col] = X_train[col].map(freq_maps[col])
                    X_test[col] = X_test[col].map(freq_maps[col]).fillna(0)
                    st.info(f"üìã {col}: {before_unique} modalit√©s ‚Üí fr√©quences [0-1]")
                
                st.success(f"‚úÖ Encodage fr√©quentiel appliqu√© √† : {', '.join(freq_cols)}")
            else:
                st.warning("‚ö†Ô∏è Aucune colonne trouv√©e pour l'encodage fr√©quentiel")

            st.markdown("---")
            
            # √âTAPE 5: Standardisation
            st.subheader(" √âtape 5 : Standardisation des Variables Num√©riques")
            
            one_hot_generated_cols = []
            if 'ohe' in locals():
                one_hot_generated_cols.extend(ohe.get_feature_names_out(ohe_cols).tolist())
            if 'ohe_typ_crb' in locals():
                one_hot_generated_cols.extend(ohe_typ_crb.get_feature_names_out(["typ_crb_grp"]).tolist())

            cols_to_exclude = one_hot_generated_cols + freq_cols
            numerical_cols = [col for col in X_train.select_dtypes(include=["int64", "float64"]).columns if col not in cols_to_exclude]

            if numerical_cols:
                st.info(f"üìä Application du StandardScaler : {', '.join(numerical_cols)}")
                st.info(f"üö´ Variables exclues : {', '.join(cols_to_exclude)}")
                
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train[numerical_cols])
                X_test_scaled = scaler.transform(X_test[numerical_cols])
                
                for i, col in enumerate(numerical_cols):
                    X_train[col] = X_train_scaled[:, i].astype('float64')
                    X_test[col] = X_test_scaled[:, i].astype('float64')
                
                st.success(f"‚úÖ Standardisation appliqu√©e √† {len(numerical_cols)} variables num√©riques")

            st.markdown("---")
            
            # √âTAPE 6: R√©sum√© final du df_final_ml propre
            st.subheader("üìä **DataFrame final pr√™t pour la mod√©lisation**")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("üìä X_train final", f"{X_train.shape[0]:,} √ó {X_train.shape[1]:,}")
            
            with col2:
                st.metric("üìä X_test final", f"{X_test.shape[0]:,} √ó {X_test.shape[1]:,}")
            
            with col3:
                st.metric("üéØ Variables finales", X_train.shape[1])
            
            with col4:
                memory_usage = (X_train.memory_usage(deep=True).sum() + X_test.memory_usage(deep=True).sum()) / 1024**2
                st.metric("üíæ Taille totale", f"{memory_usage:.1f} MB")
            
            
            colonnes_finales = pd.DataFrame({
                'Variable': X_train.columns.tolist(),
                'Type': [str(X_train[col].dtype) for col in X_train.columns],
                'Valeurs Uniques': [X_train[col].nunique() for col in X_train.columns],
                'Min': [round(X_train[col].min(), 4) if X_train[col].dtype in ['int64', 'float64'] else 'N/A' for col in X_train.columns],
                'Max': [round(X_train[col].max(), 4) if X_train[col].dtype in ['int64', 'float64'] else 'N/A' for col in X_train.columns]
            })
            
            safe_dataframe_display(colonnes_finales, "Variables finales")
            
            st.session_state['X_train_fe'] = X_train
            st.session_state['X_test_fe'] = X_test
            st.session_state['y_train_fe'] = y_train
            st.session_state['y_test_fe'] = y_test
            st.session_state['feature_columns_fe'] = X_train.columns.tolist()
            st.session_state['df_final_ml_clean'] = df_final_ml
            
            st.success("üéâ **Feature Engineering termin√© !**")

            st.markdown("---")

            # MODIFICATION: Nouvelle section r√©capitulative
                        
            st.markdown("""
            ### üéØ Points Cl√©s du Feature Engineering R√©alis√©
            
            **Nettoyage et Pr√©paration :**
            - Suppression des variables multicolin√©aires identifi√©es lors de l'analyse de corr√©lation
            - Division train/test (80/20) avant tout encodage pour √©viter le data leakage
            - Pr√©servation de l'int√©grit√© des donn√©es de test
            
            **Encodage Intelligent :**
            - **OneHotEncoder** : Variables cat√©gorielles √† faible cardinalit√© (Carrosserie, gamme)
            - **Regroupement + OneHot** : typ_crb regroup√© en 3 cat√©gories principales (GO/ES/Autres)
            - **Encodage fr√©quentiel** : Variables √† haute cardinalit√© (marques, bo√Ætes de vitesse)
            
            **Standardisation :**
            - StandardScaler appliqu√© uniquement aux variables num√©riques continues
            - Exclusion des variables encod√©es (binaires) de la standardisation
                        
            """)

        st.markdown("---")
        st.info("""
        **üöÄ Prochaines √©tapes :**
        
        Naviguez vers l'onglet **Mod√©lisation ML** pour :
        - Charger des mod√®les pr√©-entra√Æn√©s
        - Comparer les performances
        - √âvaluer diff√©rents algorithmes de ML
        - S√©lectionner le meilleur mod√®le
        """)

    
    # ----------------------------------------------------------------
    # Onglet Mod√©lisation ML (index 4) - MODIFI√â
    # ----------------------------------------------------------------
    with tabs[4]:
        st.markdown("R√©cup√©ration du dataframe post Feature Engineering et s√©lection des mod√®les adapt√©s")
        

        if 'X_train_fe' in st.session_state and st.session_state['X_train_fe'] is not None:
            X_train = st.session_state['X_train_fe']
            X_test = st.session_state['X_test_fe']
            y_train = st.session_state['y_train_fe']
            y_test = st.session_state['y_test_fe']
            feature_columns = st.session_state['feature_columns_fe']
            
            st.subheader("üìä R√©sum√© des Donn√©es pour la Mod√©lisation")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("üèãÔ∏è X_train", f"{X_train.shape[0]:,} √ó {X_train.shape[1]:,}")
            
            with col2:
                st.metric("üß™ X_test", f"{X_test.shape[0]:,} √ó {X_test.shape[1]:,}")
            
            with col3:
                st.metric("üéØ y_train", f"{y_train.shape[0]:,}")
            
            with col4:
                st.metric("üéØ y_test", f"{y_test.shape[0]:,}")
            
            st.success("‚úÖ Donn√©es pr√™tes pour la mod√©lisation - Issues du Feature Engineering avec suppression de la multicolin√©arit√©")
            
        else:
            st.info("Veuillez d'abord ex√©cuter l'onglet 'Feature Engineering' pour pr√©parer les donn√©es.")
            return

        st.markdown("---")
        
        # SECTION 1: Mod√®les Conventionnels - MODIFI√â
        st.subheader('Partie 1 : Mod√®les Conventionnels')
        
        run_conventional = st.button('üöÄ Charger les Mod√®les Conventionnels', key='run_conventional_button')
        
        if run_conventional or 'conventional_models_loaded' in st.session_state:
            if run_conventional:
                st.markdown('#### Chargement en cours...')
                
                with st.spinner('Chargement des mod√®les...'):
                    # Utilisation de la version mise en cache
                    loaded_models, loaded_scalers = load_models_and_scalers_cached('saved_models_regression')
                
                if not loaded_models:
                    st.error("‚ùå Aucun mod√®le de ML trouv√© dans le dossier 'saved_models_regression'")
                    return
                
                conventional_models = {name: model for name, model in loaded_models.items() 
                                    if 'GridSearch' not in name}
                
                if not conventional_models:
                    st.warning("‚ö†Ô∏è Aucun mod√®le conventionnel trouv√© (tous semblent √™tre des mod√®les GridSearch)")
                    conventional_models = loaded_models
                
                st.markdown("#### üìã Mod√®les conventionnels d√©tect√©s :")
                for i, model_name in enumerate(conventional_models.keys(), 1):
                    st.write(f"{i}. **{model_name}**")
                
                st.success(f'‚úÖ {len(conventional_models)} mod√®les conventionnels charg√©s avec succ√®s !')
                
                # √âvaluation parall√®le pour plus de rapidit√©
                with st.spinner('√âvaluation des mod√®les en cours...'):
                    results = evaluate_models_parallel(conventional_models, X_test, y_test)
                
                st.success('‚úÖ √âvaluation termin√©e !')
                
                st.session_state['conventional_results'] = results
                st.session_state['loaded_models'] = conventional_models
                st.session_state['conventional_models_loaded'] = True
            
            else:
                results = st.session_state.get('conventional_results', {})
                conventional_models = st.session_state.get('loaded_models', {})
            
            if results:
                st.markdown('#### R√©sultats des Mod√®les Conventionnels')
                
                results_data = []
                for name, metrics in results.items():
                    results_data.append({
                        'Mod√®le': name,
                        'MAE': round(metrics['MAE'], 4) if not np.isnan(metrics['MAE']) else 'Erreur',
                        'R¬≤': round(metrics['R2'], 4) if not np.isnan(metrics['R2']) else 'Erreur'
                    })
                
                df_results_conv = pd.DataFrame(results_data)
                
                # Tri par R¬≤ d√©croissant
                df_valid = df_results_conv[df_results_conv['R¬≤'] != 'Erreur'].copy()
                df_error = df_results_conv[df_results_conv['R¬≤'] == 'Erreur'].copy()
                
                if not df_valid.empty:
                    df_valid['R¬≤'] = df_valid['R¬≤'].astype(float)
                    df_valid = df_valid.sort_values('R¬≤', ascending=False)
                    df_valid['R¬≤'] = df_valid['R¬≤'].round(4)
                    df_final_conv = pd.concat([df_valid, df_error], ignore_index=True)
                    
                    # Identifier le meilleur mod√®le
                    best_model_conv = df_valid.iloc[0]['Mod√®le'] if not df_valid.empty else None
                else:
                    df_final_conv = df_results_conv
                    best_model_conv = None
                
                # Nouveau tableau Plotly am√©lior√©
                fig_table_conv = create_enhanced_plotly_table(
                    df_final_conv, 
                    "Performances des Mod√®les Conventionnels",
                    best_model_conv
                )
                
                st.plotly_chart(fig_table_conv, use_container_width=True)
                
                # Graphique de comparaison - NOUVEAU
                if not df_valid.empty:
                    valid_results = {k: v for k, v in results.items() if not np.isnan(v['R2'])}
                    
                    if valid_results:
                        models_names = list(valid_results.keys())
                        r2_scores = [valid_results[name]['R2'] for name in models_names]
                        mae_scores = [valid_results[name]['MAE'] for name in models_names]
                        
                        fig_comp = make_subplots(specs=[[{"secondary_y": True}]])
                        
                        # Barres R¬≤
                        fig_comp.add_trace(
                            go.Bar(
                                name='R¬≤ Score',
                                x=models_names,
                                y=r2_scores,
                                marker_color='lightblue',
                                yaxis='y'
                            ),
                            secondary_y=False
                        )
                        
                        # Ligne MAE
                        fig_comp.add_trace(
                            go.Scatter(
                                name='MAE',
                                x=models_names,
                                y=mae_scores,
                                mode='lines+markers',
                                marker_color='red',
                                line=dict(width=3),
                                yaxis='y2'
                            ),
                            secondary_y=True
                        )
                        
                        fig_comp.update_layout(
                            title='Comparaison des Performances - Mod√®les Conventionnels',
                            xaxis=dict(title='Mod√®les'),
                            height=500
                        )
                        
                        fig_comp.update_yaxes(title_text="R¬≤ Score", secondary_y=False)
                        fig_comp.update_yaxes(title_text="MAE", secondary_y=True)
                        
                        st.plotly_chart(fig_comp, use_container_width=True)

        st.markdown("---")

        # SECTION 2: Mod√®les GridSearchCV - MODIFI√â
        st.subheader('Partie 2 : Mod√®les GridSearchCV avec meilleurs hyperparam√®tres')

        @st.cache_data
        def load_hyperparameters_from_csv():
            csv_path = os.path.join('saved_models_regression', 'best_hyperparameters_with_metrics.csv')
            if os.path.exists(csv_path):
                try:
                    df_hyperparams = pd.read_csv(csv_path)
                    return df_hyperparams
                except Exception as e:
                    st.warning(f"Erreur lors du chargement du fichier CSV : {e}")
                    return None
            else:
                st.warning(f"Fichier {csv_path} non trouv√©")
                return None

        def parse_hyperparameters_from_csv(df_hyperparams, model_name):
            if df_hyperparams is None:
                return {}
            
            clean_model_name = model_name.replace("(GridSearch)", "").replace("(Grid)", "").strip()
            
            model_rows = df_hyperparams[df_hyperparams['Model'].str.contains(clean_model_name, case=False, na=False)]
            
            if model_rows.empty:
                return {}
            
            hyperparams = {}
            
            for _, row in model_rows.iterrows():
                parameter = row['Parameter']
                best_value = row['Best_Value']
                
                if parameter == 'No hyperparameters':
                    continue
                    
                if pd.isna(best_value) or best_value == '':
                    best_value = 'None (d√©faut)'
                
                hyperparams[parameter] = best_value
            
            return hyperparams
        
        run_gridsearch = st.button('üöÄ Charger les Mod√®les GridSearchCV', key='run_gridsearch_button')
        
        if run_gridsearch or 'gridsearch_models_loaded' in st.session_state:
            if run_gridsearch:
                df_hyperparams = load_hyperparameters_from_csv()
                
                with st.spinner('Chargement des mod√®les GridSearchCV...'):
                    # Utilisation de la version mise en cache
                    all_models, _ = load_models_and_scalers_cached('saved_models_regression')
                
                gridsearch_models = {name: model for name, model in all_models.items() 
                                if 'GridSearch' in name}
                
                if not gridsearch_models:
                    st.warning('‚ö†Ô∏è Aucun mod√®le GridSearchCV trouv√© dans les noms de fichiers')
                    st.info('üîÑ Recherche de mod√®les avec "gridsearch" dans le nom...')
                    
                    gridsearch_models = {name: model for name, model in all_models.items() 
                                    if 'gridsearch' in name.lower()}
                
                if gridsearch_models:
                    for i, (model_name, model) in enumerate(gridsearch_models.items(), 1):
                        st.markdown(f"**{i}. {model_name}**")
                        
                        # Cas sp√©cial pour Linear Regression (GridSearch)
                        if "Linear Regression" in model_name and "GridSearch" in model_name:
                            st.markdown("   ‚ö†Ô∏è **Hyperparam√®tres non trouv√©s**")
                        else:
                            hyperparams_csv = parse_hyperparameters_from_csv(df_hyperparams, model_name)
                            
                            if hyperparams_csv:
                                st.markdown("   üìä **Hyperparam√®tres optimis√©s :**")
                                for param, value in hyperparams_csv.items():
                                    st.markdown(f"   - **{param}** : {value}")
                            else:
                                if hasattr(model, 'best_params_') and model.best_params_:
                                    st.markdown("   üìä **Hyperparam√®tres optimis√©s (depuis mod√®le) :**")
                                    for param, value in model.best_params_.items():
                                        st.markdown(f"   - **{param}** : {value}")
                                else:
                                    clean_name = model_name.replace("(GridSearch)", "").replace("(Grid)", "").strip()
                                    st.markdown(f"   ‚ö†Ô∏è Hyperparam√®tres non trouv√©s (recherch√©: '{clean_name}')")
                                    
                                    if df_hyperparams is not None:
                                        available_models = df_hyperparams['Model'].unique()
                                        st.markdown(f"   üîç Mod√®les disponibles dans CSV: {list(available_models)}")
                        
                        st.markdown("")
                    
                    st.success(f'‚úÖ {len(gridsearch_models)} mod√®les GridSearchCV charg√©s avec succ√®s !')
                    
                    # √âvaluation parall√®le
                    with st.spinner('√âvaluation des mod√®les GridSearchCV...'):
                        gs_results_raw = evaluate_models_parallel(gridsearch_models, X_test, y_test)
                    
                    # Enrichir avec les hyperparam√®tres
                    gs_results = {}
                    for name, metrics in gs_results_raw.items():
                        hyperparams_csv = parse_hyperparameters_from_csv(df_hyperparams, name)
                        best_params = hyperparams_csv if hyperparams_csv else getattr(gridsearch_models[name], 'best_params_', {})
                        
                        gs_results[name] = {
                            'MAE': metrics['MAE'],
                            'R2': metrics['R2'],
                            'best_params': best_params,
                            'cv_score': getattr(gridsearch_models[name], 'best_score_', 'N/A')
                        }
                    
                    st.success('‚úÖ √âvaluation GridSearchCV termin√©e !')
                    
                    st.session_state['grid_search_results'] = gs_results
                    st.session_state['gridsearch_models'] = gridsearch_models
                    st.session_state['gridsearch_models_loaded'] = True
                    st.session_state['df_hyperparams'] = df_hyperparams
                
                else:
                    st.info('‚ÑπÔ∏è Aucun mod√®le GridSearchCV sp√©cifique trouv√©.')
            
            else:
                gs_results = st.session_state.get('grid_search_results', {})
                gridsearch_models = st.session_state.get('gridsearch_models', {})
                df_hyperparams = st.session_state.get('df_hyperparams', None)
            
            if gs_results:
                st.markdown('#### R√©sultats des Mod√®les GridSearchCV')
                
                gs_results_data = []
                for name, metrics in gs_results.items():
                    gs_results_data.append({
                        'Mod√®le': name,
                        'MAE': round(metrics['MAE'], 4) if not np.isnan(metrics['MAE']) else 'Erreur',
                        'R¬≤': round(metrics['R2'], 4) if not np.isnan(metrics['R2']) else 'Erreur'
                    })
                
                df_results_gs = pd.DataFrame(gs_results_data)
                
                # Tri par R¬≤ d√©croissant
                df_valid_gs = df_results_gs[df_results_gs['R¬≤'] != 'Erreur'].copy()
                df_error_gs = df_results_gs[df_results_gs['R¬≤'] == 'Erreur'].copy()
                
                if not df_valid_gs.empty:
                    df_valid_gs['R¬≤'] = df_valid_gs['R¬≤'].astype(float)
                    df_valid_gs = df_valid_gs.sort_values('R¬≤', ascending=False)
                    df_valid_gs['R¬≤'] = df_valid_gs['R¬≤'].round(4)
                    df_final_gs = pd.concat([df_valid_gs, df_error_gs], ignore_index=True)
                    
                    # Identifier le meilleur mod√®le
                    best_model_gs = df_valid_gs.iloc[0]['Mod√®le'] if not df_valid_gs.empty else None
                else:
                    df_final_gs = df_results_gs
                    best_model_gs = None
                
                # Nouveau tableau Plotly am√©lior√©
                fig_table_gs = create_enhanced_plotly_table(
                    df_final_gs, 
                    "Performances des Mod√®les GridSearchCV",
                    best_model_gs
                )
                
                st.plotly_chart(fig_table_gs, use_container_width=True)
                
                # NOUVEAU: Graphique de comparaison pour GridSearchCV
                if not df_valid_gs.empty:
                    valid_results_gs = {k: v for k, v in gs_results.items() if not np.isnan(v['R2'])}
                    
                    if valid_results_gs:
                        models_names_gs = list(valid_results_gs.keys())
                        r2_scores_gs = [valid_results_gs[name]['R2'] for name in models_names_gs]
                        mae_scores_gs = [valid_results_gs[name]['MAE'] for name in models_names_gs]
                        
                        fig_comp_gs = make_subplots(specs=[[{"secondary_y": True}]])
                        
                        # Barres R¬≤
                        fig_comp_gs.add_trace(
                            go.Bar(
                                name='R¬≤ Score',
                                x=models_names_gs,
                                y=r2_scores_gs,
                                marker_color='lightgreen',
                                yaxis='y'
                            ),
                            secondary_y=False
                        )
                        
                        # Ligne MAE
                        fig_comp_gs.add_trace(
                            go.Scatter(
                                name='MAE',
                                x=models_names_gs,
                                y=mae_scores_gs,
                                mode='lines+markers',
                                marker_color='orange',
                                line=dict(width=3),
                                yaxis='y2'
                            ),
                            secondary_y=True
                        )
                        
                        fig_comp_gs.update_layout(
                            title='Comparaison des Performances - Mod√®les GridSearchCV',
                            xaxis=dict(title='Mod√®les'),
                            height=500
                        )
                        
                        fig_comp_gs.update_yaxes(title_text="R¬≤ Score", secondary_y=False)
                        fig_comp_gs.update_yaxes(title_text="MAE", secondary_y=True)
                        
                        st.plotly_chart(fig_comp_gs, use_container_width=True)

            st.markdown("---")

        # SECTION 3: Comparaison Globale - MODIFI√â
        if ('conventional_models_loaded' in st.session_state and 'gridsearch_models_loaded' in st.session_state):
            st.subheader("Partie 3 : Comparaison Globale des Mod√®les")
                                    
            # Afficher les tableaux de performance avant la s√©lection du meilleur mod√®le
            col1, col2 = st.columns(2)
            
            # Tableau des mod√®les conventionnels
            with col1:
                if 'conventional_results' in st.session_state:
                    results = st.session_state['conventional_results']
                    results_data = []
                    for name, metrics in results.items():
                        results_data.append({
                            'Mod√®le': name,
                            'MAE': round(metrics['MAE'], 4) if not np.isnan(metrics['MAE']) else 'Erreur',
                            'R¬≤': round(metrics['R2'], 4) if not np.isnan(metrics['R2']) else 'Erreur'
                        })
                    
                    df_results_conv = pd.DataFrame(results_data)
                    
                    df_valid = df_results_conv[df_results_conv['R¬≤'] != 'Erreur'].copy()
                    df_error = df_results_conv[df_results_conv['R¬≤'] == 'Erreur'].copy()
                    
                    if not df_valid.empty:
                        df_valid['R¬≤'] = df_valid['R¬≤'].astype(float)
                        df_valid = df_valid.sort_values('R¬≤', ascending=False)
                        df_valid['R¬≤'] = df_valid['R¬≤'].round(4)
                        df_final_conv = pd.concat([df_valid, df_error], ignore_index=True)
                        
                        best_model_conv_comp = df_valid.iloc[0]['Mod√®le'] if not df_valid.empty else None
                    else:
                        df_final_conv = df_results_conv
                        best_model_conv_comp = None
                    
                    fig_table_conv_comp = create_enhanced_plotly_table(
                        df_final_conv, 
                        "Mod√®les Conventionnels",
                        best_model_conv_comp
                    )
                    fig_table_conv_comp.update_layout(height=250)
                    
                    st.plotly_chart(fig_table_conv_comp, use_container_width=True)
                else:
                    st.info("Aucun mod√®le conventionnel charg√©")
            
            # Tableau des mod√®les GridSearchCV
            with col2:
                if 'grid_search_results' in st.session_state:
                    gs_results = st.session_state['grid_search_results']
                    gs_results_data = []
                    for name, metrics in gs_results.items():
                        gs_results_data.append({
                            'Mod√®le': name,
                            'MAE': round(metrics['MAE'], 4) if not np.isnan(metrics['MAE']) else 'Erreur',
                            'R¬≤': round(metrics['R2'], 4) if not np.isnan(metrics['R2']) else 'Erreur'
                        })
                    
                    df_results_gs = pd.DataFrame(gs_results_data)
                    
                    df_valid_gs = df_results_gs[df_results_gs['R¬≤'] != 'Erreur'].copy()
                    df_error_gs = df_results_gs[df_results_gs['R¬≤'] == 'Erreur'].copy()
                    
                    if not df_valid_gs.empty:
                        df_valid_gs['R¬≤'] = df_valid_gs['R¬≤'].astype(float)
                        df_valid_gs = df_valid_gs.sort_values('R¬≤', ascending=False)
                        df_valid_gs['R¬≤'] = df_valid_gs['R¬≤'].round(4)
                        df_final_gs_comp = pd.concat([df_valid_gs, df_error_gs], ignore_index=True)
                        
                        best_model_gs_comp = df_valid_gs.iloc[0]['Mod√®le'] if not df_valid_gs.empty else None
                    else:
                        df_final_gs_comp = df_results_gs
                        best_model_gs_comp = None
                    
                    fig_table_gs_comp = create_enhanced_plotly_table(
                        df_final_gs_comp, 
                        "Mod√®les GridSearchCV",
                        best_model_gs_comp
                    )
                    fig_table_gs_comp.update_layout(height=250)
                    
                    st.plotly_chart(fig_table_gs_comp, use_container_width=True)
                else:
                    st.info("Aucun mod√®le GridSearchCV charg√©")
            
            st.markdown("---")
            
            # S√©lection du meilleur mod√®le
            st.markdown("#### üèÜ S√©lection du meilleur mod√®le")
            
            best_r2 = -np.inf
            best_model_info = None
            
            if 'conventional_results' in st.session_state:
                for model_name, metrics in st.session_state['conventional_results'].items():
                    if not np.isnan(metrics['R2']):
                        if metrics['R2'] > best_r2:
                            best_r2 = metrics['R2']
                            best_model_info = {
                                'name': model_name,
                                'type': 'Conventionnel',
                                'metrics': metrics
                            }
            
            if 'grid_search_results' in st.session_state:
                for model_name, metrics in st.session_state['grid_search_results'].items():
                    if not np.isnan(metrics['R2']):
                        if metrics['R2'] > best_r2:
                            best_r2 = metrics['R2']
                            best_model_info = {
                                'name': model_name,
                                'type': 'GridSearchCV',
                                'metrics': metrics
                            }
            
            if best_model_info:
                st.session_state['best_model_info'] = best_model_info
                
                st.success(f"üèÜ Le meilleur mod√®le est **{best_model_info['name']} ({best_model_info['type']})** avec un R¬≤ de **{best_model_info['metrics']['R2']:.4f}**")
            
            else:
                st.info("Aucun mod√®le n'a encore √©t√© charg√©. Veuillez ex√©cuter au moins une section ci-dessus.")

        else:
            st.info("Chargez d'abord des mod√®les dans les sections pr√©c√©dentes pour voir la comparaison globale.")

        st.markdown("---")
        st.info("""
        **üöÄ Prochaines √©tapes :**

        Naviguez vers l'onglet **Feature Importance** pour :
        - Analyser l'importance des variables
        - Interpr√©ter les coefficients des mod√®les
        - Comprendre les facteurs d'influence avec SHAP
        """)


    # ----------------------------------------------------------------
    # Onglet Feature Importance (index 5) - MODIFI√â
    # ----------------------------------------------------------------
    with tabs[5]:
        st.markdown("Analyse et interpr√©tation de l'importance des caract√©ristiques des mod√®les entra√Æn√©s.")
        
        if 'X_train_fe' not in st.session_state:
            st.info("Veuillez d'abord ex√©cuter l'onglet 'Feature Engineering' pour pr√©parer les donn√©es.")
            return

        X_train = st.session_state['X_train_fe']
        X_test = st.session_state['X_test_fe']
        y_train = st.session_state['y_train_fe']
        y_test = st.session_state['y_test_fe']
        feature_names = st.session_state['feature_columns_fe']

        # SECTION 1: Comparaison Feature Importance vs SHAP
        st.subheader("Etape 1 : Comparaison des M√©thodes d'Interpr√©tation")
        
        comparison_table = go.Figure(data=[go.Table(
            header=dict(
                values=["<b>Aspect</b>", "<b>Importance des Caract√©ristiques</b>", "<b>Valeurs SHAP</b>"],
                fill_color='#4a7dc4',
                font=dict(color='white', size=12),
                align='left',
                height=40
            ),
            cells=dict(
                values=[
                    ["Port√©e", "Direction", "Types de Mod√®les", "Calcul", "Interpr√©tation"],
                    ["Globale (mod√®le entier)", "Magnitude uniquement", "Principalement bas√©e sur les arbres", "Rapide", "\"Importance globale\""],
                    ["Globale + Locale (par pr√©diction)", "Magnitude + Direction", "Fonctionne pour tous les mod√®les", 
                    "Plus lent (surtout pour les grands ensembles de donn√©es)", "\"Comment/pourquoi cette pr√©diction ?\""]
                ],
                fill_color=['#e6e6e6', '#f9f9f9', '#ffffff'],
                font=dict(size=12),
                align='left',
                height=30
            )
        )])

        comparison_table.update_layout(
            title="<b>Comparison: Feature Importance vs SHAP Values</b>",
            title_x=0.5,
            margin=dict(l=20, r=20, t=60, b=20),
            width=900,
            height=350
        )

        st.plotly_chart(comparison_table, use_container_width=True)

        
        # SECTION 2: Feature Importance des Mod√®les
        st.subheader("Etape 2 : Feature Importance des Mod√®les")
        
        has_conventional = 'loaded_models' in st.session_state
        has_gridsearch = 'gridsearch_models' in st.session_state
        
        if not has_conventional and not has_gridsearch:
            st.warning("‚ö†Ô∏è Aucun mod√®le charg√© trouv√©. Veuillez d'abord charger des mod√®les dans l'onglet 'Mod√©lisation ML'.")
            return
        
        def extract_importance(model, model_name):
            if hasattr(model, 'feature_importances_'):
                return pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)
            elif hasattr(model, 'coef_'):
                return pd.Series(np.abs(model.coef_), index=feature_names).sort_values(ascending=False)
            else:
                return None
        
        importances_conv = {}
        if has_conventional:
            for name, model in st.session_state['loaded_models'].items():
                importance = extract_importance(model, name)
                if importance is not None:
                    importances_conv[f"{name} (Conv)"] = importance
        
        importances_grid = {}
        if has_gridsearch:
            for name, model in st.session_state['gridsearch_models'].items():
                importance = extract_importance(model, name)
                if importance is not None:
                    importances_grid[f"{name} (Grid)"] = importance
        
        if importances_conv or importances_grid:
            
            if importances_conv:
                st.markdown("##### üîß Mod√®les Conventionnels")
                
                n_models_conv = len([k for k in importances_conv.keys() if 'Linear' not in k])
                if n_models_conv > 0:
                    fig_conv = make_subplots(
                        rows=1, cols=min(3, n_models_conv),
                        subplot_titles=[k.replace(' (Conv)', '') for k in importances_conv.keys() if 'Linear' not in k]
                    )
                    
                    col_idx = 1
                    colors = ['royalblue', 'seagreen', 'mediumpurple']
                    
                    for i, (name, importance) in enumerate(importances_conv.items()):
                        if 'Linear' not in name and col_idx <= 3:
                            top_features = importance.head(10)
                            fig_conv.add_trace(
                                go.Bar(
                                    x=top_features.values,
                                    y=top_features.index,
                                    orientation='h',
                                    marker_color=colors[i % len(colors)],
                                    name=name.replace(' (Conv)', ''),
                                    showlegend=False
                                ),
                                row=1, col=col_idx
                            )
                            col_idx += 1
                    
                    fig_conv.update_layout(
                        title_text='Feature Importance - Mod√®les Conventionnels',
                        height=500,
                        showlegend=False
                    )
                    
                    for i in range(1, min(4, n_models_conv + 1)):
                        fig_conv.update_xaxes(title_text="Importance", row=1, col=i)
                        fig_conv.update_yaxes(title_text="Features", row=1, col=i)
                    
                    st.plotly_chart(fig_conv, use_container_width=True)
                
                linear_importance = None
                for name, importance in importances_conv.items():
                    if 'linear' in name.lower():
                        linear_importance = importance
                        break
                
                if linear_importance is not None:
                    fig_linear = go.Figure(go.Bar(
                        x=linear_importance.head(10).values,
                        y=linear_importance.head(10).index,
                        orientation='h',
                        marker_color='darkorange',
                        name='Linear Regression'
                    ))
                    
                    fig_linear.update_layout(
                        title='Coefficients - Linear Regression (Conventionnel)',
                        xaxis_title="Importance (|Coefficient|)",
                        yaxis_title="Features",
                        height=400
                    )
                    
                    st.plotly_chart(fig_linear, use_container_width=True)
            
            if importances_grid:
                st.markdown("##### üéØ Mod√®les GridSearchCV")
                
                fig_grid = make_subplots(
                    rows=1, cols=min(3, len(importances_grid)),
                    subplot_titles=[k.replace(' (Grid)', '') for k in importances_grid.keys()]
                )
                
                colors = ['royalblue', 'tomato', 'seagreen']
                
                for i, (name, importance) in enumerate(importances_grid.items()):
                    if i < 3:
                        top_features = importance.head(10)
                        fig_grid.add_trace(
                            go.Bar(
                                x=top_features.values,
                                y=top_features.index,
                                orientation='h',
                                marker_color=colors[i % len(colors)],
                                name=name.replace(' (Grid)', ''),
                                showlegend=False
                            ),
                            row=1, col=i+1
                        )
                
                fig_grid.update_layout(
                    title_text='Feature Importance - Mod√®les GridSearchCV',
                    height=500,
                    showlegend=False
                )
                
                for i in range(1, min(4, len(importances_grid) + 1)):
                    fig_grid.update_xaxes(title_text="Importance", row=1, col=i)
                    fig_grid.update_yaxes(title_text="Features", row=1, col=i)
                
                st.plotly_chart(fig_grid, use_container_width=True)
            
            st.markdown("#### üìã Tableau Comparatif des Importances")
            
            all_importances = {**importances_conv, **importances_grid}
            
            if all_importances:
                df_importances = pd.DataFrame(all_importances).fillna(0)
                
                n_rows = len(df_importances)
                n_cols = len(df_importances.columns)
                
                row_colors = ['white' if i % 2 == 0 else 'whitesmoke' for i in range(n_rows)]
                
                fill_colors = [row_colors]
                
                for col in df_importances.columns:
                    top3 = df_importances[col].nlargest(3).index.tolist()
                    col_colors = [
                        'lightgreen' if feat in top3 else base
                        for feat, base in zip(df_importances.index, row_colors)
                    ]
                    fill_colors.append(col_colors)
                
                fig_table = go.Figure(data=[
                    go.Table(
                        columnwidth=[200] + [100] * n_cols,
                        header=dict(
                            values=["<b>Feature</b>"] + [f"<b>{c}</b>" for c in df_importances.columns],
                            fill_color='darkslategray',
                            font=dict(color='white', size=14),
                            align='left'
                        ),
                        cells=dict(
                            values=[df_importances.index.tolist()] +
                                [df_importances[c].round(3).tolist() for c in df_importances.columns],
                            fill_color=fill_colors,
                            align='left'
                        )
                    )
                ])
                
                fig_table.update_layout(
                    title="Importances par mod√®le (Top 3 en vert)",
                    width=1200,
                    height=120 + 25 * n_rows,
                    margin=dict(l=20, r=20, t=60, b=20)
                )
                
                st.plotly_chart(fig_table, use_container_width=True)

        st.markdown("---")
        
        # SECTION 3: Analyse SHAP - MODIFI√â
        st.subheader("Etape 3 : Analyse SHAP pour le meilleur mod√®le")
        
        best_model_info = st.session_state.get('best_model_info')
        
        if best_model_info:
            
            best_model = None
            if best_model_info['type'] == 'GridSearchCV' and 'gridsearch_models' in st.session_state:
                best_model = st.session_state['gridsearch_models'].get(best_model_info['name'])
            elif best_model_info['type'] == 'Conventionnel' and 'loaded_models' in st.session_state:
                best_model = st.session_state['loaded_models'].get(best_model_info['name'])
            
            if best_model is None:
                st.error("Impossible de r√©cup√©rer le meilleur mod√®le")
                return
            
            # MODIFICATION: Suppression du slider et hardcoding de 500
            sample_size = 500
            st.info(f"√âchantillon fix√© √† {sample_size} observations pour l'analyse SHAP")
            
            run_shap = st.button("üöÄ Ex√©cuter l'analyse SHAP", key='run_shap_button')
            
            if run_shap:
                try:
                                        
                    with st.spinner("Calcul des valeurs SHAP..."):
                        X_test_sample = X_test.sample(n=min(sample_size, len(X_test)), random_state=42)
                        
                        explainer = shap.Explainer(
                            best_model,
                            X_train,
                            feature_perturbation="interventional"
                        )
                        
                        shap_values = explainer(X_test_sample, check_additivity=False)
                    
                    st.success("‚úÖ Valeurs SHAP calcul√©es avec succ√®s !")
                    
                    # Visualisations SHAP
                    
                    # 1. Beeswarm plot
                    fig_beeswarm, ax_beeswarm = plt.subplots(figsize=(10, 8))
                    shap.plots.beeswarm(shap_values, show=False)
                    plt.title(f"SHAP Beeswarm Plot - {best_model_info['name']}")
                    plt.tight_layout()
                    st.pyplot(fig_beeswarm)
                    
                    # 2. Bar plot
                    fig_bar, ax_bar = plt.subplots(figsize=(10, 6))
                    shap.plots.bar(shap_values, show=False)
                    plt.title(f"SHAP Bar Plot - {best_model_info['name']}")
                    plt.tight_layout()
                    st.pyplot(fig_bar)
                    
                    # 3. Analyse quantitative
                                        
                    abs_shap = np.abs(shap_values.values).mean(axis=0)
                    
                    df_shap = pd.DataFrame({
                        'feature': X_test_sample.columns,
                        'mean_abs_shap': abs_shap
                    })
                    
                    df_shap['pct'] = df_shap['mean_abs_shap'] / df_shap['mean_abs_shap'].sum() * 100
                    df_shap = df_shap.sort_values('pct', ascending=False)
                    
                    st.markdown("**Top 5 des features par importance SHAP :**")
                    top5_display = df_shap.head(5).copy()
                    top5_display['pct_formatted'] = top5_display['pct'].apply(lambda x: f"{x:.1f}%")
                    
                    safe_dataframe_display(
                        top5_display[['feature', 'pct_formatted']].rename(columns={
                            'feature': 'Feature',
                            'pct_formatted': 'Contribution (%)'
                        }),
                        "Top 5 Features SHAP"
                    )
                    
                    top3 = df_shap.head(3)
                    
                    st.markdown("#### üéØ Interpr√©tation des R√©sultats")
                    
                    st.markdown(f"""
                    **R√©sultats cl√©s de l'analyse SHAP :**
                    
                    1. **Caract√©ristique √† l'impact le plus significatif sur les pr√©dictions de CO‚ÇÇ :**
                    - **'{top3.iloc[0].feature}'** (contribution : **{top3.iloc[0].pct:.1f}%**)
                    
                    
                    2. **Autres caract√©ristiques influentes :**
                    - **{top3.iloc[1].feature}** ({top3.iloc[1].pct:.1f}%)
                    - **{top3.iloc[2].feature}** ({top3.iloc[2].pct:.1f}%)
                    
                    
                    Les valeurs SHAP nous permettent de voir non seulement **quelles** variables sont importantes, 
                    mais aussi **comment** elles influencent chaque pr√©diction individuelle dans le contexte des √©missions de CO‚ÇÇ des v√©hicules.
                    """)
                    
                    # MODIFICATION: Suppression du graphique de contribution
                    
                except Exception as e:
                    st.error(f"‚ùå Erreur lors du calcul SHAP : {e}")
                    st.info("üí° Conseil : V√©rifiez la compatibilit√© du mod√®le avec SHAP")
        
        else:
            st.info("Aucun meilleur mod√®le identifi√©. Veuillez d'abord charger des mod√®les dans l'onglet 'Mod√©lisation ML'.")
          

        st.markdown("---")

    # ----------------------------------------------------------------
    # MODIFICATION: Nouvel onglet Conclusions / Recommandations (index 6)
    # ----------------------------------------------------------------
    with tabs[6]:       
        conclusions_tabs = st.tabs(["üìä Conclusions", "üéØ Recommandations"])
        
        # Sous-onglet 1: Conclusions
        with conclusions_tabs[0]:
            st.markdown("""         
            Voici une synth√®se de notre projet de r√©gression concernant les facteurs favorisant les √©missions de CO2 
            """)        
            # Conclusions sur les donn√©es
            st.markdown("#### Qualit√© et Traitement des Donn√©es")
            
            st.markdown("""
            **‚úÖ Points forts identifi√©s :**
            - **Volume cons√©quent** : plusieurs ann√©es de donn√©es avec plus de 150 000 lignes d'observations
            - **Richesse des variables** : caract√©ristiques techniques, constructeurs, types de carburant
            - **Coh√©rence temporelle** : √©volution des √©missions observable sur la p√©riode d'√©tude
            - **Compl√©tude apr√®s traitement** : 100% de donn√©es exploitables apr√®s imputation intelligente
            
            **‚ö†Ô∏è D√©fis relev√©s :**
            - **H√©t√©rog√©n√©it√© initiale** : diff√©rences de structure entre ann√©es n√©cessitant harmonisation
            - **Multicolin√©arit√©** : variables redondantes (consommations, masses, puissances)
            - **Valeurs manquantes** : travail cons√©quent de gestion des NaN
            """)
            
            # Conclusions sur les mod√®les
            st.markdown("")
            st.markdown("#### Performance des Mod√®les de Machine Learning")
            
            if 'best_model_info' in st.session_state:
                best_info = st.session_state['best_model_info']
                st.markdown(f"""
                **üèÜ Meilleur mod√®le identifi√© :**
                - **{best_info['name']} ({best_info['type']})**
                - **R¬≤ = {best_info['metrics']['R2']:.4f}** (explique {best_info['metrics']['R2']*100:.1f}% de la variance)
                - **MAE = {best_info['metrics']['MAE']:.2f} g/km** (erreur moyenne absolue)
                """)
            
                st.markdown(f"""
                **üìà Enseignements sur les algorithmes :**
                - **Mod√®les d'ensemble** (Random Forest, Gradient Boosting) : excellentes performances gr√¢ce √† leur robustesses
                - **R√©gression lin√©aire** : performance honorable malgr√© la simplicit√©, bonne interpr√©tabilit√©
                - **Gestion des non-lin√©arit√©s** : les mod√®les bas√©s sur les arbres capturent mieux les interactions complexes
                """)
            
            # Conclusions sur les variables
            st.markdown("")
            st.markdown("#### Variables les Plus Influentes")
                       
            
            st.markdown("""
            **üéØ Facteurs techniques dominants :**
            
            L'analyse d'importance des variables (Feature Importance + SHAP) r√©v√®le que les √©missions de CO‚ÇÇ 
            sont principalement d√©termin√©es par :
            
            1. **Caract√©ristiques du moteur** : puissance, cylindr√©e, technologie
            2. **Type de carburant** : impact majeur (Diesel vs Essence vs Hybride)
            3. **Masse du v√©hicule** : corr√©lation directe avec la consommation
            4. **√âvolution temporelle** : am√©lioration technologique progressive
            5. **Segment de march√©** : diff√©rences entre constructeurs et gammes
            
            """)
            
            # Conclusions sur la m√©thodologie
            st.markdown("")
            st.markdown("#### Robustesse de la M√©thodologie")
            
            st.markdown("""
            **‚úÖ Approche rigoureuse :**
            
            1. **Preprocessing exhaustif** : nettoyage, harmonisation, imputation intelligente
            2. **Feature Engineering avanc√©** : gestion multicolin√©arit√©, encodage adaptatif
            3. **Validation crois√©e** : division train/test respect√©e, pas de data leakage
            4. **Comparaison multi-mod√®les** : √©valuation objective sur m√©triques standards
            5. **Interpr√©tabilit√©** : analyse SHAP pour comprendre les pr√©dictions
            
            **üìä M√©triques de validation :**
            - **R¬≤ √©lev√©** : Mod√®les expliquent une part significative de la variance
            - **MAE faible** : Erreurs de pr√©diction dans des ordres de grandeur acceptables
            - **Coh√©rence** : Convergence entre Feature Importance et SHAP
            """)
            
        # Sous-onglet 2: Recommandations
        with conclusions_tabs[1]:
                        
            st.markdown("""         
            Voici nos recommandations strat√©giques 
            pour les constructeurs automobiles souhaitant r√©duire l'empreinte carbone de leurs v√©hicules.
            """)
            
            # Recommandations techniques
            st.markdown("#### Optimisations Techniques Prioritaires")
            
            st.markdown("""            
            - **Downsizing moteur** : r√©duire la cylindr√©e tout en maintenant les performances
            - **Hybridation progressive** : privil√©gier des motorisations hybride sur les mod√®les existants
            - **Optimisation combustion** : technologies d'injection directe et gestion √©lectronique avanc√©e                      
            - **R√©duction masse √† vide** : privil√©gier les mat√©riaux l√©gers type aluminium, composites, aciers haute r√©sistance
            """)
            
            # Recommandations strat√©giques
            st.markdown("""#### Miser sur l'innovation et la R&D""")

            st.markdown("""
            - **Motorisations alternatives** : d√©velopper l'√©lectrique & l'hydrog√®ne pour les v√©hicules lourds.
            - **R√©cup√©ration d'√©nergie** : d√©velopper les syst√®mes KERS (Kinetic Energy Recovery System).
            - **Intelligence artificielle** : int√©grer des syst√®mes d'optimisation en temps r√©el de la combustion.
            """)
                        
           
            # Conclusion finale
            st.markdown("""#### Message Final""")
            
            st.markdown("""
            Les mod√®les pr√©dictifs d√©velopp√©s dans cette analyse fournissent une base scientifique 
            solide pour orienter les d√©cisions d'investissement et les priorit√©s de d√©veloppement produit.      
            Les constructeurs qui sauront anticiper et int√©grer rapidement les technologies de r√©duction des √©missions 
            prendront un avantage concurrentiel d√©cisif.                  
            """)
            
            # cc: 
            st.markdown("---")
            st.success(f"""
            üéâ **Analyse Compl√®te du probl√®me de r√©gression termin√©e !**           
            """)

    # Bouton de retour au d√©but
    st.markdown("---")

    if st.button("üîù Retour au d√©but", key="back_to_top_button"):
        st.markdown("""
        <script>
        window.scrollTo(0, 0);
        </script>
        """, unsafe_allow_html=True)

# Ex√©cuter l'application
if __name__ == "__main__":
    show_linear_regression()